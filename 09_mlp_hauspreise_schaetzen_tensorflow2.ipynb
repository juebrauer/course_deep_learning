{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Inhaltsverzeichnis<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Einleitung\" data-toc-modified-id=\"Einleitung-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Einleitung</a></span></li><li><span><a href=\"#Verwendeter-Datensatz\" data-toc-modified-id=\"Verwendeter-Datensatz-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Verwendeter Datensatz</a></span></li><li><span><a href=\"#Daten-einlesen\" data-toc-modified-id=\"Daten-einlesen-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Daten einlesen</a></span></li><li><span><a href=\"#Spalten-selektieren,-Daten-plotten\" data-toc-modified-id=\"Spalten-selektieren,-Daten-plotten-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Spalten selektieren, Daten plotten</a></span></li><li><span><a href=\"#Daten-normalisieren\" data-toc-modified-id=\"Daten-normalisieren-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Daten normalisieren</a></span></li><li><span><a href=\"#Trainings--und-Testdaten-definieren\" data-toc-modified-id=\"Trainings--und-Testdaten-definieren-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Trainings- und Testdaten definieren</a></span></li><li><span><a href=\"#MLP-vorbereiten\" data-toc-modified-id=\"MLP-vorbereiten-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>MLP vorbereiten</a></span></li><li><span><a href=\"#MLP-trainieren\" data-toc-modified-id=\"MLP-trainieren-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>MLP trainieren</a></span></li><li><span><a href=\"#Modell-testen/anwenden\" data-toc-modified-id=\"Modell-testen/anwenden-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Modell testen/anwenden</a></span></li><li><span><a href=\"#Modell-speichern\" data-toc-modified-id=\"Modell-speichern-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Modell speichern</a></span></li><li><span><a href=\"#Modell-wiederherstellen-und-anwenden\" data-toc-modified-id=\"Modell-wiederherstellen-und-anwenden-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Modell wiederherstellen und anwenden</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Jupyter-Notebook durchlaufen wir alle relevanten Schritte des Machine-Learnings:\n",
    "1. Daten einlesen\n",
    "2. Daten vorverarbeiten\n",
    "3. Trainings- und Testdaten vorbereiten\n",
    "4. Machine-Learning Modell definieren (hier: ein MLP)\n",
    "5. Modell trainieren\n",
    "6. Modell testen/anwenden\n",
    "7. Modell speichern/wiederherstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verwendeter Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen mit realen Daten arbeiten. Bei [Kaggle](https://www.kaggle.com/) können wir viele Datensätze finden. Diesen hier verwenden wir im Folgenden:\n",
    "\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Der Datensatz enthält in den Trainingsdaten 1460 Beispiele von Häusern, wobei deren Eigenschaften und deren jeweiliger tatsächlicher Verkaufspreis aufgeführt ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datensätze liegen often als .csv Dateien vor. Diese können mittels der Bibliothek Pandas einfach eingelesen werden.\n",
    "\n",
    "Wenn Pandas noch installiert ist, kann diese Bibliothek mittels\n",
    "\n",
    "    pip install pandas\n",
    "\n",
    "unter der Anaconda Prompt installiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"daten/hausbeispiele.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal     208500  \n",
       "1         5   2007        WD         Normal     181500  \n",
       "2         9   2008        WD         Normal     223500  \n",
       "3         2   2006        WD        Abnorml     140000  \n",
       "4        12   2008        WD         Normal     250000  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1455      8   2007        WD         Normal     175000  \n",
       "1456      2   2010        WD         Normal     210000  \n",
       "1457      5   2010        WD         Normal     266500  \n",
       "1458      4   2010        WD         Normal     142125  \n",
       "1459      6   2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 81 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "\n",
       "[3 rows x 81 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spalten selektieren, Daten plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2003\n",
       "1       1976\n",
       "2       2001\n",
       "3       1915\n",
       "4       2000\n",
       "        ... \n",
       "1455    1999\n",
       "1456    1978\n",
       "1457    1941\n",
       "1458    1950\n",
       "1459    1965\n",
       "Name: YearBuilt, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"YearBuilt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500\n",
       "1       181500\n",
       "2       223500\n",
       "3       140000\n",
       "4       250000\n",
       "         ...  \n",
       "1455    175000\n",
       "1456    210000\n",
       "1457    266500\n",
       "1458    142125\n",
       "1459    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"YearBuilt\"]\n",
    "y = df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y, marker=\"+\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = df[[\"YearBuilt\", \"LotArea\"]].values\n",
    "#data_input = df[[\"YearBuilt\", \"LotArea\", \"OverallQual\"]].values\n",
    "\n",
    "nr_inputs = data_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2003,  8450],\n",
       "       [ 1976,  9600],\n",
       "       [ 2001, 11250],\n",
       "       ...,\n",
       "       [ 1941,  9042],\n",
       "       [ 1950,  9717],\n",
       "       [ 1965,  9937]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = df[\"SalePrice\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([208500, 181500, 223500, ..., 266500, 142125, 147500])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_input = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input_data = scaler_input.fit_transform(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94927536, 0.0334198 ],\n",
       "       [0.75362319, 0.03879502],\n",
       "       [0.93478261, 0.04650728],\n",
       "       ...,\n",
       "       [0.5       , 0.03618687],\n",
       "       [0.56521739, 0.03934189],\n",
       "       [0.67391304, 0.04037019]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaled_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_output = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_output_data = scaler_output.fit_transform(data_output.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24107763],\n",
       "       [0.20358284],\n",
       "       [0.26190807],\n",
       "       ...,\n",
       "       [0.321622  ],\n",
       "       [0.14890293],\n",
       "       [0.15636717]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings- und Testdaten definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaled_input_data[0:1000]\n",
    "y_train = scaled_output_data[0:1000]\n",
    "x_test  = scaled_input_data[1000:]\n",
    "y_test  = scaled_output_data[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94927536 0.0334198 ] --> [0.24107763]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0], \"-->\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(10,\n",
    "                             activation=\"relu\",\n",
    "                             input_shape=(nr_inputs,)))\n",
    "model.add(keras.layers.Dense(1,\n",
    "                             activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',               \n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 0s 412us/sample - loss: 0.0431 - accuracy: 0.0010\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 0.0155 - accuracy: 0.0010\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.0121 - accuracy: 0.0010\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 0.0117 - accuracy: 0.0010\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 0.0116 - accuracy: 0.0010\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 0.0115 - accuracy: 0.0010\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 0.0114 - accuracy: 0.0010\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 0.0114 - accuracy: 0.0010\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 0.0113 - accuracy: 0.0010\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 0s 59us/sample - loss: 0.0112 - accuracy: 0.0010\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0112 - accuracy: 0.0010\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 0.0111 - accuracy: 0.0010\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.0110 - accuracy: 0.0010\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 0.0110 - accuracy: 0.0010\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 0s 204us/sample - loss: 0.0109 - accuracy: 0.0010\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 0s 192us/sample - loss: 0.0109 - accuracy: 0.0010\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 0s 108us/sample - loss: 0.0108 - accuracy: 0.0010\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 0.0108 - accuracy: 0.0010\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 0.0107 - accuracy: 0.0010\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 0s 176us/sample - loss: 0.0107 - accuracy: 0.0010\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 0s 122us/sample - loss: 0.0106 - accuracy: 0.0010\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 0s 106us/sample - loss: 0.0106 - accuracy: 0.0010\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 0s 107us/sample - loss: 0.0105 - accuracy: 0.0010\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 0.0105 - accuracy: 0.0010\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 0.0105 - accuracy: 0.0010: 0s - loss: 0.0106 - accuracy: 0.0000\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 0.0104 - accuracy: 0.0010\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0104 - accuracy: 0.0010\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.0104 - accuracy: 0.0010\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 0s 243us/sample - loss: 0.0103 - accuracy: 0.0010: 0s - loss: 0.0107 - accuracy: 0.00\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 0s 123us/sample - loss: 0.0103 - accuracy: 0.0010\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 0s 130us/sample - loss: 0.0103 - accuracy: 0.0010\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 0.0102 - accuracy: 0.0010\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 0s 118us/sample - loss: 0.0102 - accuracy: 0.0010\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 0.0102 - accuracy: 0.0010\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 0s 118us/sample - loss: 0.0101 - accuracy: 0.0010\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 0.0101 - accuracy: 0.0010\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 0.0101 - accuracy: 0.0010\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 0s 210us/sample - loss: 0.0101 - accuracy: 0.0010\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 0.0100 - accuracy: 0.0010\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 0s 202us/sample - loss: 0.0100 - accuracy: 0.0010\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 0s 293us/sample - loss: 0.0100 - accuracy: 0.0010\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 0.0100 - accuracy: 0.0010\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.0100 - accuracy: 0.0010\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.0099 - accuracy: 0.0010\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 0s 66us/sample - loss: 0.0099 - accuracy: 0.0010\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 0.0099 - accuracy: 0.0010\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 0.0099 - accuracy: 0.0010\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 0s 192us/sample - loss: 0.0099 - accuracy: 0.0010\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 0s 143us/sample - loss: 0.0099 - accuracy: 0.0010\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 0s 118us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 0s 111us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 0s 152us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 0s 130us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 0s 111us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 0s 108us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 0s 125us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 0s 194us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 0s 118us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 0s 100us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 0s 106us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 0s 200us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 0s 104us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 0s 123us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 0s 110us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 0s 157us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 0s 175us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 0s 111us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 0s 118us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 0s 103us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 0s 182us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 0s 165us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 0s 169us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 0s 109us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 0s 175us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 0s 107us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 0s 127us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 0s 171us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 0s 157us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0093 - accuracy: 0.0010A: 0s - loss: 0.0088 - accuracy: 0.0000e+\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 0s 108us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 0s 187us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 0s 128us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 0s 106us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 0s 157us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 0s 185us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 0s 127us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 0s 173us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 0s 127us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 0s 127us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 0s 130us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 0s 177us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 0s 59us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 0s 177us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 0s 103us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 0s 106us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 0s 101us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 0s 98us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 0s 103us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 0s 114us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 0s 143us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 0s 156us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 0s 172us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 0s 125us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 0s 167us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 0s 114us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 0s 185us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 0s 125us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 0s 143us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 0.0091 - accuracy: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell testen/anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16794443],\n",
       "       [0.10868789],\n",
       "       [0.2686156 ],\n",
       "       [0.21263239],\n",
       "       [0.26711538]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dollar = scaler_output.inverse_transform( preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_dollar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[155836.78],\n",
       "       [113166.15],\n",
       "       [228330.1 ],\n",
       "       [188016.58],\n",
       "       [227249.78],\n",
       "       [189599.89],\n",
       "       [179896.75],\n",
       "       [180657.2 ],\n",
       "       [225627.88],\n",
       "       [121189.94]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_dollar[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dollar = scaler_output.inverse_transform( y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dollar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 82000.],\n",
       "       [ 86000.],\n",
       "       [232000.],\n",
       "       [136905.],\n",
       "       [181000.],\n",
       "       [149900.],\n",
       "       [163500.],\n",
       "       [ 88000.],\n",
       "       [240000.],\n",
       "       [102000.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dollar[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155836.78] vs [82000.] --> Fehler: [-73836.78125]\n",
      "[113166.15] vs [86000.] --> Fehler: [-27166.1484375]\n",
      "[228330.1] vs [232000.] --> Fehler: [3669.90625]\n",
      "[188016.58] vs [136905.] --> Fehler: [-51111.578125]\n",
      "[227249.78] vs [181000.] --> Fehler: [-46249.78125]\n",
      "[189599.89] vs [149900.] --> Fehler: [-39699.890625]\n",
      "[179896.75] vs [163500.] --> Fehler: [-16396.75]\n",
      "[180657.2] vs [88000.] --> Fehler: [-92657.203125]\n",
      "[225627.88] vs [240000.] --> Fehler: [14372.125]\n",
      "[121189.94] vs [102000.] --> Fehler: [-19189.9375]\n",
      "[149609.16] vs [135000.] --> Fehler: [-14609.15625]\n",
      "[173350.28] vs [100000.] --> Fehler: [-73350.28125]\n",
      "[116803.72] vs [165000.] --> Fehler: [48196.28125]\n",
      "[99578.93] vs [85000.] --> Fehler: [-14578.9296875]\n",
      "[150345.44] vs [119200.] --> Fehler: [-31145.4375]\n",
      "[221877.52] vs [227000.] --> Fehler: [5122.484375]\n",
      "[214886.27] vs [203000.] --> Fehler: [-11886.265625]\n",
      "[199211.25] vs [187500.] --> Fehler: [-11711.25]\n",
      "[208245.61] vs [160000.] --> Fehler: [-48245.609375]\n",
      "[227253.45] vs [213490.] --> Fehler: [-13763.453125]\n",
      "[227166.31] vs [176000.] --> Fehler: [-51166.3125]\n",
      "[228432.77] vs [194000.] --> Fehler: [-34432.765625]\n",
      "[126306.586] vs [87000.] --> Fehler: [-39306.5859375]\n",
      "[227249.78] vs [191000.] --> Fehler: [-36249.78125]\n",
      "[187710.66] vs [287000.] --> Fehler: [99289.34375]\n",
      "[182927.55] vs [112500.] --> Fehler: [-70427.546875]\n",
      "[166665.72] vs [167500.] --> Fehler: [834.28125]\n",
      "[229661.56] vs [293077.] --> Fehler: [63415.4375]\n",
      "[141096.98] vs [105000.] --> Fehler: [-36096.984375]\n",
      "[183388.19] vs [118000.] --> Fehler: [-65388.1875]\n",
      "[107657.664] vs [160000.] --> Fehler: [52342.3359375]\n",
      "[112365.54] vs [197000.] --> Fehler: [84634.4609375]\n",
      "[210648.05] vs [310000.] --> Fehler: [99351.953125]\n",
      "[223243.52] vs [230000.] --> Fehler: [6756.484375]\n",
      "[137306.] vs [119750.] --> Fehler: [-17556.]\n",
      "[162462.53] vs [84000.] --> Fehler: [-78462.53125]\n",
      "[229588.19] vs [315500.] --> Fehler: [85911.8125]\n",
      "[221813.25] vs [287000.] --> Fehler: [65186.75]\n",
      "[180709.53] vs [97000.] --> Fehler: [-83709.53125]\n",
      "[180713.83] vs [80000.] --> Fehler: [-100713.828125]\n",
      "[162338.19] vs [155000.] --> Fehler: [-7338.1875]\n",
      "[174748.42] vs [173000.] --> Fehler: [-1748.421875]\n",
      "[227202.02] vs [196000.] --> Fehler: [-31202.015625]\n",
      "[206819.94] vs [262280.] --> Fehler: [55460.0625]\n",
      "[194886.69] vs [278000.] --> Fehler: [83113.3125]\n",
      "[159605.81] vs [139600.] --> Fehler: [-20005.8125]\n",
      "[226671.48] vs [556581.] --> Fehler: [329909.515625]\n",
      "[212398.22] vs [145000.] --> Fehler: [-67398.21875]\n",
      "[165713.08] vs [115000.] --> Fehler: [-50713.078125]\n",
      "[147698.69] vs [84900.] --> Fehler: [-62798.6875]\n",
      "[229673.02] vs [176485.] --> Fehler: [-53188.015625]\n",
      "[229625.61] vs [200141.] --> Fehler: [-29484.609375]\n",
      "[172030.22] vs [165000.] --> Fehler: [-7030.21875]\n",
      "[162687.33] vs [144500.] --> Fehler: [-18187.328125]\n",
      "[222995.42] vs [255000.] --> Fehler: [32004.578125]\n",
      "[188027.2] vs [180000.] --> Fehler: [-8027.203125]\n",
      "[227165.7] vs [185850.] --> Fehler: [-41315.703125]\n",
      "[210813.22] vs [248000.] --> Fehler: [37186.78125]\n",
      "[230897.47] vs [335000.] --> Fehler: [104102.53125]\n",
      "[128856.] vs [220000.] --> Fehler: [91144.]\n",
      "[222113.05] vs [213500.] --> Fehler: [-8613.046875]\n",
      "[132376.28] vs [81000.] --> Fehler: [-51376.28125]\n",
      "[85639.7] vs [90000.] --> Fehler: [4360.296875]\n",
      "[119844.984] vs [110500.] --> Fehler: [-9344.984375]\n",
      "[174605.34] vs [154000.] --> Fehler: [-20605.34375]\n",
      "[214724.25] vs [328000.] --> Fehler: [113275.75]\n",
      "[211161.02] vs [178000.] --> Fehler: [-33161.015625]\n",
      "[172010.33] vs [167900.] --> Fehler: [-4110.328125]\n",
      "[184558.38] vs [151400.] --> Fehler: [-33158.375]\n",
      "[151848.33] vs [135000.] --> Fehler: [-16848.328125]\n",
      "[161220.72] vs [135000.] --> Fehler: [-26220.71875]\n",
      "[177241.69] vs [154000.] --> Fehler: [-23241.6875]\n",
      "[150657.56] vs [91500.] --> Fehler: [-59157.5625]\n",
      "[189633.17] vs [159500.] --> Fehler: [-30133.171875]\n",
      "[228407.77] vs [194000.] --> Fehler: [-34407.765625]\n",
      "[139474.05] vs [219500.] --> Fehler: [80025.953125]\n",
      "[134272.16] vs [170000.] --> Fehler: [35727.84375]\n",
      "[178267.55] vs [138800.] --> Fehler: [-39467.546875]\n",
      "[225947.81] vs [155900.] --> Fehler: [-70047.8125]\n",
      "[212434.19] vs [126000.] --> Fehler: [-86434.1875]\n",
      "[181327.02] vs [145000.] --> Fehler: [-36327.015625]\n",
      "[170838.3] vs [133000.] --> Fehler: [-37838.296875]\n",
      "[223195.77] vs [192000.] --> Fehler: [-31195.765625]\n",
      "[172083.78] vs [160000.] --> Fehler: [-12083.78125]\n",
      "[213453.48] vs [187500.] --> Fehler: [-25953.484375]\n",
      "[209721.78] vs [147000.] --> Fehler: [-62721.78125]\n",
      "[184710.64] vs [83500.] --> Fehler: [-101210.640625]\n",
      "[227089.2] vs [252000.] --> Fehler: [24910.796875]\n",
      "[225989.4] vs [137500.] --> Fehler: [-88489.40625]\n",
      "[227246.88] vs [197000.] --> Fehler: [-30246.875]\n",
      "[153274.06] vs [92900.] --> Fehler: [-60374.0625]\n",
      "[219623.53] vs [160000.] --> Fehler: [-59623.53125]\n",
      "[119661.34] vs [136500.] --> Fehler: [16838.65625]\n",
      "[173395.83] vs [146000.] --> Fehler: [-27395.828125]\n",
      "[161548.52] vs [129000.] --> Fehler: [-32548.515625]\n",
      "[228391.22] vs [176432.] --> Fehler: [-51959.21875]\n",
      "[104983.055] vs [127000.] --> Fehler: [22016.9453125]\n",
      "[202063.22] vs [170000.] --> Fehler: [-32063.21875]\n",
      "[134639.44] vs [128000.] --> Fehler: [-6639.4375]\n",
      "[190677.38] vs [157000.] --> Fehler: [-33677.375]\n",
      "[112936.59] vs [60000.] --> Fehler: [-52936.59375]\n",
      "[181425.11] vs [119500.] --> Fehler: [-61925.109375]\n",
      "[166841.72] vs [135000.] --> Fehler: [-31841.71875]\n",
      "[165350.61] vs [159500.] --> Fehler: [-5850.609375]\n",
      "[180672.58] vs [106000.] --> Fehler: [-74672.578125]\n",
      "[212167.83] vs [325000.] --> Fehler: [112832.171875]\n",
      "[206933.33] vs [179900.] --> Fehler: [-27033.328125]\n",
      "[227465.45] vs [274725.] --> Fehler: [47259.546875]\n",
      "[220558.36] vs [181000.] --> Fehler: [-39558.359375]\n",
      "[225685.72] vs [280000.] --> Fehler: [54314.28125]\n",
      "[213838.42] vs [188000.] --> Fehler: [-25838.421875]\n",
      "[188094.64] vs [205000.] --> Fehler: [16905.359375]\n",
      "[162799.2] vs [129900.] --> Fehler: [-32899.203125]\n",
      "[157279.92] vs [134500.] --> Fehler: [-22779.921875]\n",
      "[158894.44] vs [117000.] --> Fehler: [-41894.4375]\n",
      "[229605.84] vs [318000.] --> Fehler: [88394.15625]\n",
      "[223272.2] vs [184100.] --> Fehler: [-39172.203125]\n",
      "[176044.86] vs [130000.] --> Fehler: [-46044.859375]\n",
      "[163629.58] vs [140000.] --> Fehler: [-23629.578125]\n",
      "[165453.9] vs [133700.] --> Fehler: [-31753.90625]\n",
      "[112947.08] vs [118400.] --> Fehler: [5452.921875]\n",
      "[227099.84] vs [212900.] --> Fehler: [-14199.84375]\n",
      "[161314.53] vs [112000.] --> Fehler: [-49314.53125]\n",
      "[149173.34] vs [118000.] --> Fehler: [-31173.34375]\n",
      "[209717.48] vs [163900.] --> Fehler: [-45817.484375]\n",
      "[159854.17] vs [115000.] --> Fehler: [-44854.171875]\n",
      "[229788.38] vs [174000.] --> Fehler: [-55788.375]\n",
      "[225440.1] vs [259000.] --> Fehler: [33559.90625]\n",
      "[225652.52] vs [215000.] --> Fehler: [-10652.515625]\n",
      "[193725.42] vs [140000.] --> Fehler: [-53725.421875]\n",
      "[123741.8] vs [135000.] --> Fehler: [11258.203125]\n",
      "[208251.1] vs [93500.] --> Fehler: [-114751.09375]\n",
      "[59023.832] vs [117500.] --> Fehler: [58476.16796875]\n",
      "[213698.56] vs [239500.] --> Fehler: [25801.4375]\n",
      "[216469.2] vs [169000.] --> Fehler: [-47469.203125]\n",
      "[121176.164] vs [102000.] --> Fehler: [-19176.1640625]\n",
      "[153193.27] vs [119000.] --> Fehler: [-34193.265625]\n",
      "[52706.53] vs [94000.] --> Fehler: [41293.46875]\n",
      "[189490.16] vs [196000.] --> Fehler: [6509.84375]\n",
      "[112911.27] vs [144000.] --> Fehler: [31088.7265625]\n",
      "[154710.38] vs [139000.] --> Fehler: [-15710.375]\n",
      "[188108.1] vs [197500.] --> Fehler: [9391.90625]\n",
      "[228377.14] vs [424870.] --> Fehler: [196492.859375]\n",
      "[165343.72] vs [80000.] --> Fehler: [-85343.71875]\n",
      "[140891.31] vs [80000.] --> Fehler: [-60891.3125]\n",
      "[123861.48] vs [149000.] --> Fehler: [25138.5234375]\n",
      "[200144.08] vs [180000.] --> Fehler: [-20144.078125]\n",
      "[140905.08] vs [174500.] --> Fehler: [33594.921875]\n",
      "[121212.9] vs [116900.] --> Fehler: [-4312.8984375]\n",
      "[112890.695] vs [143000.] --> Fehler: [30109.3046875]\n",
      "[153294.25] vs [124000.] --> Fehler: [-29294.25]\n",
      "[164673.81] vs [149900.] --> Fehler: [-14773.8125]\n",
      "[160917.47] vs [230000.] --> Fehler: [69082.53125]\n",
      "[126578.16] vs [120500.] --> Fehler: [-6078.15625]\n",
      "[173053.78] vs [201800.] --> Fehler: [28746.21875]\n",
      "[188072.6] vs [218000.] --> Fehler: [29927.40625]\n",
      "[173386.66] vs [179900.] --> Fehler: [6513.34375]\n",
      "[229759.77] vs [230000.] --> Fehler: [240.234375]\n",
      "[229609.16] vs [235128.] --> Fehler: [5518.84375]\n",
      "[185508.78] vs [185000.] --> Fehler: [-508.78125]\n",
      "[191411.98] vs [146000.] --> Fehler: [-45411.984375]\n",
      "[158176.86] vs [224000.] --> Fehler: [65823.140625]\n",
      "[177469.38] vs [129000.] --> Fehler: [-48469.375]\n",
      "[178494.8] vs [108959.] --> Fehler: [-69535.796875]\n",
      "[190350.12] vs [194000.] --> Fehler: [3649.875]\n",
      "[232210.6] vs [233170.] --> Fehler: [959.40625]\n",
      "[230915.56] vs [245350.] --> Fehler: [14434.4375]\n",
      "[220344.95] vs [173000.] --> Fehler: [-47344.953125]\n",
      "[132703.17] vs [235000.] --> Fehler: [102296.828125]\n",
      "[211714.3] vs [625000.] --> Fehler: [413285.703125]\n",
      "[189485.5] vs [171000.] --> Fehler: [-18485.5]\n",
      "[163989.58] vs [163000.] --> Fehler: [-989.578125]\n",
      "[228506.38] vs [171900.] --> Fehler: [-56606.375]\n",
      "[147168.39] vs [200500.] --> Fehler: [53331.609375]\n",
      "[128451.59] vs [239000.] --> Fehler: [110548.40625]\n",
      "[209598.66] vs [285000.] --> Fehler: [75401.34375]\n",
      "[199124.23] vs [119500.] --> Fehler: [-79624.234375]\n",
      "[121346.81] vs [115000.] --> Fehler: [-6346.8125]\n",
      "[114336.56] vs [154900.] --> Fehler: [40563.4375]\n",
      "[158669.88] vs [93000.] --> Fehler: [-65669.875]\n",
      "[206871.12] vs [250000.] --> Fehler: [43128.875]\n",
      "[231021.75] vs [392500.] --> Fehler: [161478.25]\n",
      "[214600.08] vs [745000.] --> Fehler: [530399.921875]\n",
      "[112752.96] vs [120000.] --> Fehler: [7247.0390625]\n",
      "[168723.89] vs [186700.] --> Fehler: [17976.109375]\n",
      "[118214.016] vs [104900.] --> Fehler: [-13314.015625]\n",
      "[85868.12] vs [95000.] --> Fehler: [9131.8828125]\n",
      "[212152.14] vs [262000.] --> Fehler: [49847.859375]\n",
      "[223181.53] vs [195000.] --> Fehler: [-28181.53125]\n",
      "[219256.48] vs [189000.] --> Fehler: [-30256.484375]\n",
      "[166238.27] vs [168000.] --> Fehler: [1761.734375]\n",
      "[219613.] vs [174000.] --> Fehler: [-45613.]\n",
      "[119569.52] vs [125000.] --> Fehler: [5430.4765625]\n",
      "[219486.03] vs [165000.] --> Fehler: [-54486.03125]\n",
      "[178765.36] vs [158000.] --> Fehler: [-20765.359375]\n",
      "[227144.48] vs [176000.] --> Fehler: [-51144.484375]\n",
      "[228169.64] vs [219210.] --> Fehler: [-8959.640625]\n",
      "[107522.375] vs [144000.] --> Fehler: [36477.625]\n",
      "[221823.95] vs [178000.] --> Fehler: [-43823.953125]\n",
      "[170552.5] vs [148000.] --> Fehler: [-22552.5]\n",
      "[180111.17] vs [116050.] --> Fehler: [-64061.171875]\n",
      "[217689.64] vs [197900.] --> Fehler: [-19789.640625]\n",
      "[119844.984] vs [117000.] --> Fehler: [-2844.984375]\n",
      "[220429.27] vs [213000.] --> Fehler: [-7429.265625]\n",
      "[186775.69] vs [153500.] --> Fehler: [-33275.6875]\n",
      "[206602.22] vs [271900.] --> Fehler: [65297.78125]\n",
      "[174766.03] vs [107000.] --> Fehler: [-67766.03125]\n",
      "[224511.17] vs [200000.] --> Fehler: [-24511.171875]\n",
      "[169473.22] vs [140000.] --> Fehler: [-29473.21875]\n",
      "[228372.44] vs [290000.] --> Fehler: [61627.5625]\n",
      "[209557.33] vs [189000.] --> Fehler: [-20557.328125]\n",
      "[204107.44] vs [164000.] --> Fehler: [-40107.4375]\n",
      "[141108.62] vs [113000.] --> Fehler: [-28108.625]\n",
      "[173318.08] vs [145000.] --> Fehler: [-28318.078125]\n",
      "[169286.38] vs [134500.] --> Fehler: [-34786.375]\n",
      "[174904.2] vs [125000.] --> Fehler: [-49904.203125]\n",
      "[190903.11] vs [112000.] --> Fehler: [-78903.109375]\n",
      "[232230.19] vs [229456.] --> Fehler: [-2774.1875]\n",
      "[149415.52] vs [80500.] --> Fehler: [-68915.515625]\n",
      "[182043.25] vs [91500.] --> Fehler: [-90543.25]\n",
      "[172160.3] vs [115000.] --> Fehler: [-57160.296875]\n",
      "[177505.67] vs [134000.] --> Fehler: [-43505.671875]\n",
      "[151779.75] vs [143000.] --> Fehler: [-8779.75]\n",
      "[154455.58] vs [137900.] --> Fehler: [-16555.578125]\n",
      "[225377.97] vs [184000.] --> Fehler: [-41377.96875]\n",
      "[163885.36] vs [145000.] --> Fehler: [-18885.359375]\n",
      "[229472.97] vs [214000.] --> Fehler: [-15472.96875]\n",
      "[173423.23] vs [147000.] --> Fehler: [-26423.234375]\n",
      "[230952.62] vs [367294.] --> Fehler: [136341.375]\n",
      "[166772.08] vs [127000.] --> Fehler: [-39772.078125]\n",
      "[188796.05] vs [190000.] --> Fehler: [1203.953125]\n",
      "[169475.9] vs [132500.] --> Fehler: [-36975.90625]\n",
      "[169314.14] vs [101800.] --> Fehler: [-67514.140625]\n",
      "[165101.92] vs [142000.] --> Fehler: [-23101.921875]\n",
      "[100822.48] vs [130000.] --> Fehler: [29177.5234375]\n",
      "[104504.81] vs [138887.] --> Fehler: [34382.1875]\n",
      "[224712.34] vs [175500.] --> Fehler: [-49212.34375]\n",
      "[225606.84] vs [195000.] --> Fehler: [-30606.84375]\n",
      "[226899.83] vs [142500.] --> Fehler: [-84399.828125]\n",
      "[228397.31] vs [265900.] --> Fehler: [37502.6875]\n",
      "[224585.94] vs [224900.] --> Fehler: [314.0625]\n",
      "[229654.4] vs [248328.] --> Fehler: [18673.59375]\n",
      "[185393.62] vs [170000.] --> Fehler: [-15393.625]\n",
      "[228182.11] vs [465000.] --> Fehler: [236817.890625]\n",
      "[124808.92] vs [230000.] --> Fehler: [105191.078125]\n",
      "[198731.03] vs [178000.] --> Fehler: [-20731.03125]\n",
      "[227142.4] vs [186500.] --> Fehler: [-40642.40625]\n",
      "[187953.2] vs [169900.] --> Fehler: [-18053.203125]\n",
      "[108809.93] vs [129500.] --> Fehler: [20690.0703125]\n",
      "[153376.9] vs [119000.] --> Fehler: [-34376.90625]\n",
      "[177283.] vs [244000.] --> Fehler: [66717.]\n",
      "[224701.3] vs [171750.] --> Fehler: [-52951.296875]\n",
      "[177382.61] vs [130000.] --> Fehler: [-47382.609375]\n",
      "[184864.33] vs [294000.] --> Fehler: [109135.671875]\n",
      "[224618.84] vs [165400.] --> Fehler: [-59218.84375]\n",
      "[127896.33] vs [127500.] --> Fehler: [-396.328125]\n",
      "[212011.17] vs [301500.] --> Fehler: [89488.828125]\n",
      "[115958.586] vs [99900.] --> Fehler: [-16058.5859375]\n",
      "[227110.64] vs [190000.] --> Fehler: [-37110.640625]\n",
      "[178735.84] vs [151000.] --> Fehler: [-27735.84375]\n",
      "[217941.75] vs [181000.] --> Fehler: [-36941.75]\n",
      "[161262.95] vs [128900.] --> Fehler: [-32362.953125]\n",
      "[162481.64] vs [161500.] --> Fehler: [-981.640625]\n",
      "[111200.26] vs [180500.] --> Fehler: [69299.7421875]\n",
      "[218174.77] vs [181000.] --> Fehler: [-37174.765625]\n",
      "[219544.58] vs [183900.] --> Fehler: [-35644.578125]\n",
      "[99355.49] vs [122000.] --> Fehler: [22644.5078125]\n",
      "[230856.03] vs [378500.] --> Fehler: [147643.96875]\n",
      "[132674.69] vs [381000.] --> Fehler: [248325.3125]\n",
      "[163819.39] vs [144000.] --> Fehler: [-19819.390625]\n",
      "[191125.94] vs [260000.] --> Fehler: [68874.0625]\n",
      "[177436.33] vs [185750.] --> Fehler: [8313.671875]\n",
      "[173067.11] vs [137000.] --> Fehler: [-36067.109375]\n",
      "[165151.5] vs [177000.] --> Fehler: [11848.5]\n",
      "[99719.56] vs [139000.] --> Fehler: [39280.4375]\n",
      "[150369.86] vs [137000.] --> Fehler: [-13369.859375]\n",
      "[182526.9] vs [162000.] --> Fehler: [-20526.90625]\n",
      "[175424.52] vs [197900.] --> Fehler: [22475.484375]\n",
      "[223140.38] vs [237000.] --> Fehler: [13859.625]\n",
      "[113005.46] vs [68400.] --> Fehler: [-44605.4609375]\n",
      "[223114.73] vs [227000.] --> Fehler: [3885.265625]\n",
      "[207109.94] vs [180000.] --> Fehler: [-27109.9375]\n",
      "[189568.12] vs [150500.] --> Fehler: [-39068.125]\n",
      "[181452.52] vs [139000.] --> Fehler: [-42452.515625]\n",
      "[111496.92] vs [169000.] --> Fehler: [57503.078125]\n",
      "[138674.27] vs [132500.] --> Fehler: [-6174.265625]\n",
      "[170663.06] vs [143000.] --> Fehler: [-27663.0625]\n",
      "[169964.22] vs [190000.] --> Fehler: [20035.78125]\n",
      "[220741.94] vs [278000.] --> Fehler: [57258.0625]\n",
      "[228353.25] vs [281000.] --> Fehler: [52646.75]\n",
      "[171677.3] vs [180500.] --> Fehler: [8822.703125]\n",
      "[183388.19] vs [119500.] --> Fehler: [-63888.1875]\n",
      "[75415.74] vs [107500.] --> Fehler: [32084.2578125]\n",
      "[188120.64] vs [162900.] --> Fehler: [-25220.640625]\n",
      "[160027.25] vs [115000.] --> Fehler: [-45027.25]\n",
      "[177494.19] vs [138500.] --> Fehler: [-38994.1875]\n",
      "[170746.48] vs [155000.] --> Fehler: [-15746.484375]\n",
      "[227239.08] vs [140000.] --> Fehler: [-87239.078125]\n",
      "[227046.39] vs [160000.] --> Fehler: [-67046.390625]\n",
      "[165458.5] vs [154000.] --> Fehler: [-11458.5]\n",
      "[219006.88] vs [225000.] --> Fehler: [5993.125]\n",
      "[142594.36] vs [177500.] --> Fehler: [34905.640625]\n",
      "[212331.27] vs [290000.] --> Fehler: [77668.734375]\n",
      "[227130.17] vs [232000.] --> Fehler: [4869.828125]\n",
      "[225971.1] vs [130000.] --> Fehler: [-95971.09375]\n",
      "[228237.05] vs [325000.] --> Fehler: [96762.953125]\n",
      "[227167.81] vs [202500.] --> Fehler: [-24667.8125]\n",
      "[212487.97] vs [138000.] --> Fehler: [-74487.96875]\n",
      "[150319.72] vs [147000.] --> Fehler: [-3319.71875]\n",
      "[208523.44] vs [179200.] --> Fehler: [-29323.4375]\n",
      "[164693.31] vs [335000.] --> Fehler: [170306.6875]\n",
      "[227127.42] vs [203000.] --> Fehler: [-24127.421875]\n",
      "[206993.4] vs [302000.] --> Fehler: [95006.59375]\n",
      "[218699.89] vs [333168.] --> Fehler: [114468.109375]\n",
      "[158680.94] vs [119000.] --> Fehler: [-39680.9375]\n",
      "[178634.44] vs [206900.] --> Fehler: [28265.5625]\n",
      "[230920.97] vs [295493.] --> Fehler: [64572.03125]\n",
      "[228501.77] vs [208900.] --> Fehler: [-19601.765625]\n",
      "[221389.27] vs [275000.] --> Fehler: [53610.734375]\n",
      "[158526.] vs [111000.] --> Fehler: [-47526.]\n",
      "[162699.73] vs [156500.] --> Fehler: [-6199.734375]\n",
      "[152075.81] vs [72500.] --> Fehler: [-79575.8125]\n",
      "[209636.3] vs [190000.] --> Fehler: [-19636.296875]\n",
      "[140070.5] vs [82500.] --> Fehler: [-57570.5]\n",
      "[228376.7] vs [147000.] --> Fehler: [-81376.703125]\n",
      "[115991.04] vs [55000.] --> Fehler: [-60991.0390625]\n",
      "[128047.06] vs [79000.] --> Fehler: [-49047.0625]\n",
      "[196461.22] vs [130500.] --> Fehler: [-65961.21875]\n",
      "[112780.5] vs [256000.] --> Fehler: [143219.5]\n",
      "[217790.34] vs [176500.] --> Fehler: [-41290.34375]\n",
      "[228376.39] vs [227000.] --> Fehler: [-1376.390625]\n",
      "[188071.67] vs [132500.] --> Fehler: [-55571.671875]\n",
      "[137109.2] vs [100000.] --> Fehler: [-37109.203125]\n",
      "[137237.52] vs [125500.] --> Fehler: [-11737.515625]\n",
      "[180645.64] vs [125000.] --> Fehler: [-55645.640625]\n",
      "[189503.08] vs [167900.] --> Fehler: [-21603.078125]\n",
      "[184154.2] vs [135000.] --> Fehler: [-49154.203125]\n",
      "[141508.19] vs [52500.] --> Fehler: [-89008.1875]\n",
      "[222836.05] vs [200000.] --> Fehler: [-22836.046875]\n",
      "[182785.2] vs [128500.] --> Fehler: [-54285.203125]\n",
      "[181537.14] vs [123000.] --> Fehler: [-58537.140625]\n",
      "[224162.25] vs [155000.] --> Fehler: [-69162.25]\n",
      "[223147.86] vs [228500.] --> Fehler: [5352.140625]\n",
      "[123760.63] vs [177000.] --> Fehler: [53239.3671875]\n",
      "[228352.44] vs [155835.] --> Fehler: [-72517.4375]\n",
      "[113120.25] vs [108500.] --> Fehler: [-4620.25]\n",
      "[176546.81] vs [262500.] --> Fehler: [85953.1875]\n",
      "[228073.83] vs [283463.] --> Fehler: [55389.171875]\n",
      "[217246.14] vs [215000.] --> Fehler: [-2246.140625]\n",
      "[51442.668] vs [122000.] --> Fehler: [70557.33203125]\n",
      "[178590.98] vs [200000.] --> Fehler: [21409.015625]\n",
      "[169359.67] vs [171000.] --> Fehler: [1640.328125]\n",
      "[135984.38] vs [134900.] --> Fehler: [-1084.375]\n",
      "[213324.23] vs [410000.] --> Fehler: [196675.765625]\n",
      "[220385.97] vs [235000.] --> Fehler: [14614.03125]\n",
      "[177357.05] vs [170000.] --> Fehler: [-7357.046875]\n",
      "[174721.88] vs [110000.] --> Fehler: [-64721.875]\n",
      "[181212.48] vs [149900.] --> Fehler: [-31312.484375]\n",
      "[220899.22] vs [177500.] --> Fehler: [-43399.21875]\n",
      "[225274.44] vs [315000.] --> Fehler: [89725.5625]\n",
      "[114171.2] vs [189000.] --> Fehler: [74828.796875]\n",
      "[226663.7] vs [260000.] --> Fehler: [33336.296875]\n",
      "[112621.875] vs [104900.] --> Fehler: [-7721.875]\n",
      "[228409.] vs [156932.] --> Fehler: [-71477.]\n",
      "[227249.83] vs [144152.] --> Fehler: [-83097.828125]\n",
      "[220601.42] vs [216000.] --> Fehler: [-4601.421875]\n",
      "[219128.03] vs [193000.] --> Fehler: [-26128.03125]\n",
      "[190037.58] vs [127000.] --> Fehler: [-63037.578125]\n",
      "[224673.1] vs [144000.] --> Fehler: [-80673.09375]\n",
      "[224396.4] vs [232000.] --> Fehler: [7603.59375]\n",
      "[113166.15] vs [105000.] --> Fehler: [-8166.1484375]\n",
      "[159918.] vs [165500.] --> Fehler: [5582.]\n",
      "[217739.39] vs [274300.] --> Fehler: [56560.609375]\n",
      "[221647.98] vs [466500.] --> Fehler: [244852.015625]\n",
      "[227087.08] vs [250000.] --> Fehler: [22912.921875]\n",
      "[229629.61] vs [239000.] --> Fehler: [9370.390625]\n",
      "[126547.39] vs [91000.] --> Fehler: [-35547.390625]\n",
      "[140981.75] vs [117000.] --> Fehler: [-23981.75]\n",
      "[184712.25] vs [83000.] --> Fehler: [-101712.25]\n",
      "[228382.14] vs [167500.] --> Fehler: [-60882.140625]\n",
      "[104881.29] vs [58500.] --> Fehler: [-46381.2890625]\n",
      "[179837.84] vs [237500.] --> Fehler: [57662.15625]\n",
      "[113028.43] vs [157000.] --> Fehler: [43971.5703125]\n",
      "[108950.55] vs [112000.] --> Fehler: [3049.453125]\n",
      "[138440.14] vs [105000.] --> Fehler: [-33440.140625]\n",
      "[115853.305] vs [125500.] --> Fehler: [9646.6953125]\n",
      "[190309.19] vs [250000.] --> Fehler: [59690.8125]\n",
      "[107547.625] vs [136000.] --> Fehler: [28452.375]\n",
      "[228105.52] vs [377500.] --> Fehler: [149394.484375]\n",
      "[141364.17] vs [131000.] --> Fehler: [-10364.171875]\n",
      "[220479.02] vs [235000.] --> Fehler: [14520.984375]\n",
      "[176107.61] vs [124000.] --> Fehler: [-52107.609375]\n",
      "[176192.23] vs [123000.] --> Fehler: [-53192.234375]\n",
      "[92578.71] vs [163000.] --> Fehler: [70421.2890625]\n",
      "[228505.77] vs [246578.] --> Fehler: [18072.234375]\n",
      "[226930.97] vs [281213.] --> Fehler: [54282.03125]\n",
      "[146861.08] vs [160000.] --> Fehler: [13138.921875]\n",
      "[113111.06] vs [137500.] --> Fehler: [24388.9375]\n",
      "[153376.9] vs [138000.] --> Fehler: [-15376.90625]\n",
      "[119831.91] vs [137450.] --> Fehler: [17618.09375]\n",
      "[125224.78] vs [120000.] --> Fehler: [-5224.78125]\n",
      "[225883.06] vs [193000.] --> Fehler: [-32883.0625]\n",
      "[228446.73] vs [193879.] --> Fehler: [-34567.734375]\n",
      "[229422.61] vs [282922.] --> Fehler: [53499.390625]\n",
      "[106058.04] vs [105000.] --> Fehler: [-1058.0390625]\n",
      "[225960.7] vs [275000.] --> Fehler: [49039.296875]\n",
      "[182870.53] vs [133000.] --> Fehler: [-49870.53125]\n",
      "[200329.25] vs [112000.] --> Fehler: [-88329.25]\n",
      "[99537.61] vs [125500.] --> Fehler: [25962.390625]\n",
      "[200774.03] vs [215000.] --> Fehler: [14225.96875]\n",
      "[221569.92] vs [230000.] --> Fehler: [8430.078125]\n",
      "[153193.27] vs [140000.] --> Fehler: [-13193.265625]\n",
      "[152031.97] vs [90000.] --> Fehler: [-62031.96875]\n",
      "[227058.83] vs [257000.] --> Fehler: [29941.171875]\n",
      "[116615.41] vs [207000.] --> Fehler: [90384.59375]\n",
      "[229789.42] vs [175900.] --> Fehler: [-53889.421875]\n",
      "[65638.4] vs [122500.] --> Fehler: [56861.6015625]\n",
      "[217219.42] vs [340000.] --> Fehler: [122780.578125]\n",
      "[170707.92] vs [124000.] --> Fehler: [-46707.921875]\n",
      "[178228.44] vs [223000.] --> Fehler: [44771.5625]\n",
      "[177241.69] vs [179900.] --> Fehler: [2658.3125]\n",
      "[189932.14] vs [127500.] --> Fehler: [-62432.140625]\n",
      "[224673.1] vs [136500.] --> Fehler: [-88173.09375]\n",
      "[173940.39] vs [274970.] --> Fehler: [101029.609375]\n",
      "[163960.28] vs [144000.] --> Fehler: [-19960.28125]\n",
      "[165212.05] vs [142000.] --> Fehler: [-23212.046875]\n",
      "[212268.22] vs [271000.] --> Fehler: [58731.78125]\n",
      "[146366.75] vs [140000.] --> Fehler: [-6366.75]\n",
      "[139927.4] vs [119000.] --> Fehler: [-20927.40625]\n",
      "[194661.27] vs [182900.] --> Fehler: [-11761.265625]\n",
      "[226222.03] vs [192140.] --> Fehler: [-34082.03125]\n",
      "[188519.45] vs [143750.] --> Fehler: [-44769.453125]\n",
      "[122167.61] vs [64500.] --> Fehler: [-57667.609375]\n",
      "[220390.17] vs [186500.] --> Fehler: [-33890.171875]\n",
      "[188910.08] vs [160000.] --> Fehler: [-28910.078125]\n",
      "[169424.48] vs [174000.] --> Fehler: [4575.515625]\n",
      "[181483.12] vs [120500.] --> Fehler: [-60983.125]\n",
      "[230872.78] vs [394617.] --> Fehler: [163744.21875]\n",
      "[162775.72] vs [149700.] --> Fehler: [-13075.71875]\n",
      "[192044.98] vs [197000.] --> Fehler: [4955.015625]\n",
      "[115387.3] vs [191000.] --> Fehler: [75612.703125]\n",
      "[225948.] vs [149300.] --> Fehler: [-76648.]\n",
      "[230904.1] vs [310000.] --> Fehler: [79095.90625]\n",
      "[107522.06] vs [121000.] --> Fehler: [13477.9375]\n",
      "[225859.5] vs [179600.] --> Fehler: [-46259.5]\n",
      "[174804.28] vs [129000.] --> Fehler: [-45804.28125]\n",
      "[168066.89] vs [157900.] --> Fehler: [-10166.890625]\n",
      "[213685.4] vs [240000.] --> Fehler: [26314.59375]\n",
      "[99229.46] vs [112000.] --> Fehler: [12770.5390625]\n",
      "[180709.53] vs [92000.] --> Fehler: [-88709.53125]\n",
      "[185517.97] vs [136000.] --> Fehler: [-49517.96875]\n",
      "[230941.92] vs [287090.] --> Fehler: [56148.078125]\n",
      "[227239.08] vs [145000.] --> Fehler: [-82239.078125]\n",
      "[227927.6] vs [84500.] --> Fehler: [-143427.59375]\n",
      "[225881.23] vs [185000.] --> Fehler: [-40881.234375]\n",
      "[219224.58] vs [175000.] --> Fehler: [-44224.578125]\n",
      "[190578.31] vs [210000.] --> Fehler: [19421.6875]\n",
      "[141131.4] vs [266500.] --> Fehler: [125368.59375]\n",
      "[153184.31] vs [142125.] --> Fehler: [-11059.3125]\n",
      "[173341.72] vs [147500.] --> Fehler: [-25841.71875]\n",
      "Durchschnittlicher Fehler in $: [46229.33381454]\n"
     ]
    }
   ],
   "source": [
    "nr_tests = len(y_test)\n",
    "sum_errors = 0.0\n",
    "for i in range(0,nr_tests):\n",
    "    error = gt_dollar[i] - preds_dollar[i]\n",
    "    print(\"{0} vs {1} --> Fehler: {2}\"\n",
    "          .format(preds_dollar[i],\n",
    "                  gt_dollar[i],\n",
    "                  error ))\n",
    "    sum_errors += abs(error)\n",
    "print(\"Durchschnittlicher Fehler in $:\", sum_errors/nr_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname1 = \"hauspreis_schaetzer.h5\"\n",
    "model.save(fname1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fname2 = \"scaler_output.pkl\"\n",
    "fobj = open(fname2, \"wb\")\n",
    "pickle.dump(scaler_output, fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell wiederherstellen und anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model(fname1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fobj = open(fname2, \"rb\")\n",
    "scaler = pickle.load(fobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.preprocessing._data.MinMaxScaler"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = np.array( [[0.94927536, 0.0334198 ]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict( test_sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2633851]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[224563.6]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.inverse_transform( pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Inhalte",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Jupyter-Notebook durchlaufen wir alle relevanten Schritte des Machine-Learnings:\n",
    "1. Daten einlesen\n",
    "2. Daten vorverarbeiten\n",
    "3. Trainings- und Testdaten vorbereiten\n",
    "4. Machine-Learning Modell definieren (hier: ein MLP)\n",
    "5. Modell trainieren\n",
    "6. Modell testen/anwenden\n",
    "7. Modell speichern/wiederherstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verwendeter Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen mit realen Daten arbeiten. Bei [Kaggle](https://www.kaggle.com/) können wir viele Datensätze finden. Diesen hier verwenden wir im Folgenden:\n",
    "\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Der Datensatz enthält in den Trainingsdaten 1460 Beispiele von Häusern, wobei deren Eigenschaften und deren jeweiliger tatsächlicher Verkaufspreis aufgeführt ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datensätze liegen often als .csv Dateien vor. Diese können mittels der Bibliothek Pandas einfach eingelesen werden.\n",
    "\n",
    "Wenn Pandas noch nicht installiert ist, kann diese Bibliothek mittels\n",
    "\n",
    "    conda install pandas\n",
    "\n",
    "unter der Anaconda Prompt installiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"daten/hausbeispiele.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal     208500  \n",
       "1         5   2007        WD         Normal     181500  \n",
       "2         9   2008        WD         Normal     223500  \n",
       "3         2   2006        WD        Abnorml     140000  \n",
       "4        12   2008        WD         Normal     250000  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1455      8   2007        WD         Normal     175000  \n",
       "1456      2   2010        WD         Normal     210000  \n",
       "1457      5   2010        WD         Normal     266500  \n",
       "1458      4   2010        WD         Normal     142125  \n",
       "1459      6   2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 81 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Reg\n",
       "1       Reg\n",
       "2       IR1\n",
       "3       IR1\n",
       "4       IR1\n",
       "       ... \n",
       "1455    Reg\n",
       "1456    Reg\n",
       "1457    Reg\n",
       "1458    Reg\n",
       "1459    Reg\n",
       "Name: LotShape, Length: 1460, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"LotShape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Reg', 'IR1', 'IR2', 'IR3'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"LotShape\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>119.0</td>\n",
       "      <td>790</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n",
       "0        1          60         65.0     8450            7            5   \n",
       "1        2          20         80.0     9600            6            8   \n",
       "2        3          60         68.0    11250            7            5   \n",
       "3        4          70         60.0     9550            7            5   \n",
       "4        5          60         84.0    14260            8            5   \n",
       "...    ...         ...          ...      ...          ...          ...   \n",
       "1455  1456          60         62.0     7917            6            5   \n",
       "1456  1457          20         85.0    13175            6            6   \n",
       "1457  1458          70         66.0     9042            7            9   \n",
       "1458  1459          20         68.0     9717            5            6   \n",
       "1459  1460          20         75.0     9937            5            6   \n",
       "\n",
       "      YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  SaleType_ConLw  \\\n",
       "0          2003          2003       196.0         706  ...               0   \n",
       "1          1976          1976         0.0         978  ...               0   \n",
       "2          2001          2002       162.0         486  ...               0   \n",
       "3          1915          1970         0.0         216  ...               0   \n",
       "4          2000          2000       350.0         655  ...               0   \n",
       "...         ...           ...         ...         ...  ...             ...   \n",
       "1455       1999          2000         0.0           0  ...               0   \n",
       "1456       1978          1988       119.0         790  ...               0   \n",
       "1457       1941          2006         0.0         275  ...               0   \n",
       "1458       1950          1996         0.0          49  ...               0   \n",
       "1459       1965          1965         0.0         830  ...               0   \n",
       "\n",
       "      SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0                0             0            1                      0   \n",
       "1                0             0            1                      0   \n",
       "2                0             0            1                      0   \n",
       "3                0             0            1                      1   \n",
       "4                0             0            1                      0   \n",
       "...            ...           ...          ...                    ...   \n",
       "1455             0             0            1                      0   \n",
       "1456             0             0            1                      0   \n",
       "1457             0             0            1                      0   \n",
       "1458             0             0            1                      0   \n",
       "1459             0             0            1                      0   \n",
       "\n",
       "      SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                         0                     0                     0   \n",
       "1                         0                     0                     0   \n",
       "2                         0                     0                     0   \n",
       "3                         0                     0                     0   \n",
       "4                         0                     0                     0   \n",
       "...                     ...                   ...                   ...   \n",
       "1455                      0                     0                     0   \n",
       "1456                      0                     0                     0   \n",
       "1457                      0                     0                     0   \n",
       "1458                      0                     0                     0   \n",
       "1459                      0                     0                     0   \n",
       "\n",
       "      SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                        1                      0  \n",
       "1                        1                      0  \n",
       "2                        1                      0  \n",
       "3                        0                      0  \n",
       "4                        1                      0  \n",
       "...                    ...                    ...  \n",
       "1455                     1                      0  \n",
       "1456                     1                      0  \n",
       "1457                     1                      0  \n",
       "1458                     1                      0  \n",
       "1459                     1                      0  \n",
       "\n",
       "[1460 rows x 290 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "\n",
       "[3 rows x 81 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spalten selektieren, Daten plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2003\n",
       "1       1976\n",
       "2       2001\n",
       "3       1915\n",
       "4       2000\n",
       "        ... \n",
       "1455    1999\n",
       "1456    1978\n",
       "1457    1941\n",
       "1458    1950\n",
       "1459    1965\n",
       "Name: YearBuilt, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"YearBuilt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500\n",
       "1       181500\n",
       "2       223500\n",
       "3       140000\n",
       "4       250000\n",
       "         ...  \n",
       "1455    175000\n",
       "1456    210000\n",
       "1457    266500\n",
       "1458    142125\n",
       "1459    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"YearBuilt\"]\n",
    "y = df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y, marker=\"+\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = df[[\"YearBuilt\", \"LotArea\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_input = df[[\"YearBuilt\", \"LotArea\"]].values\n",
    "data_input = df[[\"YearBuilt\", \"LotArea\", \"OverallQual\"]].values\n",
    "\n",
    "nr_inputs = data_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2003,  8450,     7],\n",
       "       [ 1976,  9600,     6],\n",
       "       [ 2001, 11250,     7],\n",
       "       ...,\n",
       "       [ 1941,  9042,     7],\n",
       "       [ 1950,  9717,     5],\n",
       "       [ 1965,  9937,     5]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = df[\"SalePrice\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([208500, 181500, 223500, ..., 266500, 142125, 147500])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_input = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input_data = scaler_input.fit_transform(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94927536, 0.0334198 , 0.66666667],\n",
       "       [0.75362319, 0.03879502, 0.55555556],\n",
       "       [0.93478261, 0.04650728, 0.66666667],\n",
       "       ...,\n",
       "       [0.5       , 0.03618687, 0.66666667],\n",
       "       [0.56521739, 0.03934189, 0.44444444],\n",
       "       [0.67391304, 0.04037019, 0.44444444]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaled_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_input_data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_output = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_output_data = scaler_output.fit_transform(data_output.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24107763],\n",
       "       [0.20358284],\n",
       "       [0.26190807],\n",
       "       ...,\n",
       "       [0.321622  ],\n",
       "       [0.14890293],\n",
       "       [0.15636717]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings- und Testdaten definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaled_input_data[0:1000]\n",
    "y_train = scaled_output_data[0:1000]\n",
    "x_test  = scaled_input_data[1000:]\n",
    "y_test  = scaled_output_data[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97101449 0.02552993 0.66666667] --> [0.23760589]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[-1], \"-->\", y_train[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "# 1 2 3\n",
    "# 4 5 6\n",
    "# 7 8 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M[:][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2 = np.array([[1,2,3], [4,5,6], [7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 9])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 9])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2[:][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nr_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-baf0c9b19ccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m model.add(keras.layers.Dense(80,\n\u001b[1;32m      9\u001b[0m                              \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                              input_shape=(nr_inputs,)))\n\u001b[0m\u001b[1;32m     11\u001b[0m model.add(keras.layers.Dense(40,\n\u001b[1;32m     12\u001b[0m                              \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nr_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# 123\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(80,\n",
    "                             activation=\"relu\",\n",
    "                             input_shape=(nr_inputs,)))\n",
    "model.add(keras.layers.Dense(40,\n",
    "                             activation=\"relu\"\n",
    "                             ))\n",
    "model.add(keras.layers.Dense(1,\n",
    "                             activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',           \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#loss=tf.keras.losses.MeanSquaredError(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 3,601\n",
      "Trainable params: 3,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "32/32 [==============================] - 1s 11ms/step - loss: 0.0337 - accuracy: 5.1468e-04\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0099 - accuracy: 2.2427e-04\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0086 - accuracy: 9.1153e-05\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.0029\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 6.7284e-04\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 2.6069e-04\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 6.1713e-04\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.0014\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0071 - accuracy: 0.0019\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0087 - accuracy: 9.3564e-04\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0065 - accuracy: 0.0013\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0064 - accuracy: 4.2224e-04\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0058 - accuracy: 7.9515e-04\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 4.6733e-04\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.0014\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.0025\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 2.2427e-04\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 0.0014\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 7.9515e-04\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0063 - accuracy: 0.0039\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 0.0029\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0056 - accuracy: 4.2224e-04\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0056 - accuracy: 0.0025\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 0.0014\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 1.8919e-04\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 0.0019\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.2272e-04\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 0.0011\n",
      "Epoch 29/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 4.6733e-04\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 2.6069e-04\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.0010\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 0.0029\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 0.0013\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 0.0014\n",
      "Epoch 35/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 3.3802e-04\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 7.3202e-04\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 1.5537e-04\n",
      "Epoch 38/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 6.1713e-04\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.0029\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0047 - accuracy: 7.9515e-04: 0s - loss: 0.0046 - accuracy: 5.8601e-\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 6.1713e-04\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 1.8919e-04\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 6.1713e-04\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.0019\n",
      "Epoch 45/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 3.7920e-04\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 9.1153e-05\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0041 - accuracy: 2.9857e-04\n",
      "Epoch 48/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 3.3802e-04\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 2.2427e-04\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 1.5537e-04\n",
      "Epoch 51/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.0019\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 7.3202e-04\n",
      "Epoch 53/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 6.0606e-05\n",
      "Epoch 54/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 2.9857e-04\n",
      "Epoch 55/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0019\n",
      "Epoch 56/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0041 - accuracy: 0.0010\n",
      "Epoch 57/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0040 - accuracy: 4.2224e-04\n",
      "Epoch 58/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 2.2427e-04\n",
      "Epoch 59/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0037 - accuracy: 1.8919e-04\n",
      "Epoch 60/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0041 - accuracy: 6.1713e-04\n",
      "Epoch 61/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.0025\n",
      "Epoch 62/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 2.2427e-04\n",
      "Epoch 63/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 2.6069e-04\n",
      "Epoch 64/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0044 - accuracy: 0.0019\n",
      "Epoch 65/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0039 - accuracy: 0.0012\n",
      "Epoch 66/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 5.6452e-04\n",
      "Epoch 67/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 0.0012\n",
      "Epoch 68/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0012\n",
      "Epoch 69/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0053 - accuracy: 1.2272e-04\n",
      "Epoch 70/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 7.3202e-04\n",
      "Epoch 71/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0035 - accuracy: 0.0010\n",
      "Epoch 72/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 7.3202e-04\n",
      "Epoch 73/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.0021\n",
      "Epoch 74/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 2.6069e-04\n",
      "Epoch 75/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 7.3202e-04\n",
      "Epoch 76/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.0017\n",
      "Epoch 77/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 2.9857e-04\n",
      "Epoch 78/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0033 - accuracy: 5.6452e-04\n",
      "Epoch 79/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.0025\n",
      "Epoch 80/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.0010\n",
      "Epoch 81/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.0014\n",
      "Epoch 82/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 5.6452e-04\n",
      "Epoch 83/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0040 - accuracy: 6.7284e-04\n",
      "Epoch 84/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 2.6069e-04\n",
      "Epoch 85/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 0.0021\n",
      "Epoch 86/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 0.0013\n",
      "Epoch 87/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 0.0039\n",
      "Epoch 88/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0035 - accuracy: 0.0039\n",
      "Epoch 89/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0044 - accuracy: 9.3564e-04\n",
      "Epoch 90/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.0013\n",
      "Epoch 91/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 2.6069e-04\n",
      "Epoch 92/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 6.7284e-04\n",
      "Epoch 93/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0040 - accuracy: 2.2427e-04\n",
      "Epoch 94/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0057 - accuracy: 0.0025\n",
      "Epoch 95/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 0.0011\n",
      "Epoch 96/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0045 - accuracy: 0.0010\n",
      "Epoch 97/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0039 - accuracy: 6.1713e-04\n",
      "Epoch 98/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 8.6279e-04\n",
      "Epoch 99/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 9.1153e-05\n",
      "Epoch 100/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 1.5537e-04\n",
      "Epoch 101/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 6.1713e-04\n",
      "Epoch 102/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 3.7920e-04\n",
      "Epoch 103/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0021\n",
      "Epoch 104/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 6.1713e-04\n",
      "Epoch 105/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0043 - accuracy: 0.0019\n",
      "Epoch 106/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 6.7284e-04\n",
      "Epoch 107/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 2.9857e-04\n",
      "Epoch 108/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0032 - accuracy: 1.2272e-04\n",
      "Epoch 109/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 9.3564e-04\n",
      "Epoch 110/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 6.1713e-04\n",
      "Epoch 111/200\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0034 - accuracy: 3.3802e-04\n",
      "Epoch 112/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 5.6452e-04\n",
      "Epoch 113/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0038 - accuracy: 2.9857e-04\n",
      "Epoch 114/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 3.7920e-04\n",
      "Epoch 115/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 4.2224e-04\n",
      "Epoch 116/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0044 - accuracy: 0.0021\n",
      "Epoch 117/200\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.0040 - accuracy: 7.9515e-04\n",
      "Epoch 118/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 0.0014\n",
      "Epoch 119/200\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0037 - accuracy: 3.3802e-04\n",
      "Epoch 120/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0040 - accuracy: 7.3202e-04\n",
      "Epoch 121/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 1.8919e-04\n",
      "Epoch 122/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0035 - accuracy: 0.0017\n",
      "Epoch 123/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0040 - accuracy: 0.0013\n",
      "Epoch 124/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0014\n",
      "Epoch 125/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.0011\n",
      "Epoch 126/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0045 - accuracy: 0.0017\n",
      "Epoch 127/200\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0032 - accuracy: 9.1153e-05\n",
      "Epoch 128/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 3.7920e-04\n",
      "Epoch 129/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0032 - accuracy: 3.7920e-04\n",
      "Epoch 130/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0040 - accuracy: 2.9857e-04\n",
      "Epoch 131/200\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0052 - accuracy: 6.1713e-04\n",
      "Epoch 132/200\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0040 - accuracy: 0.0014\n",
      "Epoch 133/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0033 - accuracy: 0.0011\n",
      "Epoch 134/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 0.0010\n",
      "Epoch 135/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0034 - accuracy: 1.2272e-04\n",
      "Epoch 136/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 7.3202e-04\n",
      "Epoch 137/200\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.0033 - accuracy: 6.1713e-04\n",
      "Epoch 138/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 8.6279e-04\n",
      "Epoch 139/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0035 - accuracy: 2.6069e-04\n",
      "Epoch 140/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0034 - accuracy: 9.1153e-05\n",
      "Epoch 141/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 7.3202e-04\n",
      "Epoch 142/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 5.1468e-04\n",
      "Epoch 143/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0038 - accuracy: 0.0029\n",
      "Epoch 144/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0029 - accuracy: 6.7284e-04\n",
      "Epoch 145/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0040 - accuracy: 1.2272e-04\n",
      "Epoch 146/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 9.1153e-05\n",
      "Epoch 147/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0019\n",
      "Epoch 148/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 0.0025\n",
      "Epoch 149/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 5.1468e-04\n",
      "Epoch 150/200\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0035 - accuracy: 2.2427e-04\n",
      "Epoch 151/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 7.9515e-04\n",
      "Epoch 152/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0033 - accuracy: 2.2427e-04\n",
      "Epoch 153/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0045 - accuracy: 4.2224e-04\n",
      "Epoch 154/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0038 - accuracy: 5.6452e-04\n",
      "Epoch 155/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0035 - accuracy: 4.2224e-04\n",
      "Epoch 156/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 5.6452e-04\n",
      "Epoch 157/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 0.0016\n",
      "Epoch 158/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.0021\n",
      "Epoch 159/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0041 - accuracy: 0.0016\n",
      "Epoch 160/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0019\n",
      "Epoch 161/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 1.2272e-04\n",
      "Epoch 162/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 5.1468e-04\n",
      "Epoch 163/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0041 - accuracy: 0.0021\n",
      "Epoch 164/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 7.9515e-04\n",
      "Epoch 165/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 6.1713e-04\n",
      "Epoch 166/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0035 - accuracy: 0.0011\n",
      "Epoch 167/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0036 - accuracy: 6.1713e-04\n",
      "Epoch 168/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0037 - accuracy: 0.0017\n",
      "Epoch 169/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 0.0039\n",
      "Epoch 170/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0036 - accuracy: 0.0011\n",
      "Epoch 171/200\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0035 - accuracy: 2.6069e-04\n",
      "Epoch 172/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 2.9857e-04\n",
      "Epoch 173/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0039 - accuracy: 0.0025\n",
      "Epoch 174/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 6.2216e- - 0s 6ms/step - loss: 0.0031 - accuracy: 7.3202e-04\n",
      "Epoch 175/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0037 - accuracy: 6.7284e-04\n",
      "Epoch 176/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 3.7920e-04\n",
      "Epoch 177/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 1.2272e-04\n",
      "Epoch 178/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 0.0021\n",
      "Epoch 179/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0033 - accuracy: 9.1153e-05\n",
      "Epoch 180/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.0012\n",
      "Epoch 181/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0034 - accuracy: 0.0013\n",
      "Epoch 182/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 6.7284e-04\n",
      "Epoch 183/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.0019\n",
      "Epoch 184/200\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0035 - accuracy: 0.0019\n",
      "Epoch 185/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0034 - accuracy: 1.2272e-04\n",
      "Epoch 186/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.0010\n",
      "Epoch 187/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0033 - accuracy: 5.6452e-04\n",
      "Epoch 188/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 5.6452e-04\n",
      "Epoch 189/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 7.9515e-04\n",
      "Epoch 190/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0034 - accuracy: 1.2272e-04\n",
      "Epoch 191/200\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0040 - accuracy: 0.0016\n",
      "Epoch 192/200\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.0037 - accuracy: 1.5537e-04\n",
      "Epoch 193/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 0.0011\n",
      "Epoch 194/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0033 - accuracy: 3.7920e-04\n",
      "Epoch 195/200\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0043 - accuracy: 7.9515e-04\n",
      "Epoch 196/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0032 - accuracy: 0.0012\n",
      "Epoch 197/200\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0036 - accuracy: 0.0029\n",
      "Epoch 198/200\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0044 - accuracy: 0.0021\n",
      "Epoch 199/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 3.3802e-04\n",
      "Epoch 200/200\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.0041 - accuracy: 9.1153e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.02094261161983013,\n",
       "  0.009276525117456913,\n",
       "  0.00879687163978815,\n",
       "  0.0083505529910326,\n",
       "  0.008104544132947922,\n",
       "  0.007876964285969734,\n",
       "  0.007644551806151867,\n",
       "  0.007421522866934538,\n",
       "  0.007213834207504988,\n",
       "  0.007039167452603579,\n",
       "  0.006871999241411686,\n",
       "  0.006710486952215433,\n",
       "  0.006577055901288986,\n",
       "  0.006442836485803127,\n",
       "  0.006317541468888521,\n",
       "  0.006181186530739069,\n",
       "  0.006073099095374346,\n",
       "  0.005966348573565483,\n",
       "  0.005858303979039192,\n",
       "  0.005764019675552845,\n",
       "  0.005675881635397673,\n",
       "  0.005572837311774492,\n",
       "  0.0054674032144248486,\n",
       "  0.0053788647055625916,\n",
       "  0.0053023057989776134,\n",
       "  0.0052297692745924,\n",
       "  0.0051584322936832905,\n",
       "  0.0050943936221301556,\n",
       "  0.0050387075170874596,\n",
       "  0.004977087490260601,\n",
       "  0.004919140599668026,\n",
       "  0.004870897624641657,\n",
       "  0.004816368687897921,\n",
       "  0.004772351123392582,\n",
       "  0.004727365914732218,\n",
       "  0.004681778606027365,\n",
       "  0.004651650786399841,\n",
       "  0.00461111543700099,\n",
       "  0.004578114952892065,\n",
       "  0.00454215332865715,\n",
       "  0.004512393847107887,\n",
       "  0.0044829705730080605,\n",
       "  0.004464049357920885,\n",
       "  0.004430086817592382,\n",
       "  0.004405818413943052,\n",
       "  0.004390995483845472,\n",
       "  0.004365842789411545,\n",
       "  0.004343281500041485,\n",
       "  0.004319772589951754,\n",
       "  0.004302346147596836,\n",
       "  0.0042836423963308334,\n",
       "  0.004267167765647173,\n",
       "  0.00425516115501523,\n",
       "  0.004236176609992981,\n",
       "  0.004214160609990358,\n",
       "  0.004201371688395739,\n",
       "  0.004188131541013718,\n",
       "  0.004170131869614124,\n",
       "  0.004167126026004553,\n",
       "  0.004143191035836935,\n",
       "  0.004133231472223997,\n",
       "  0.004125061444938183,\n",
       "  0.004120705183595419,\n",
       "  0.00409926800057292,\n",
       "  0.00410213228315115,\n",
       "  0.004075903445482254,\n",
       "  0.004077994264662266,\n",
       "  0.00405954010784626,\n",
       "  0.004063691012561321,\n",
       "  0.004054631572216749,\n",
       "  0.004038678947836161,\n",
       "  0.004034283105283976,\n",
       "  0.004013136960566044,\n",
       "  0.0040254355408251286,\n",
       "  0.0040178741328418255,\n",
       "  0.0040092975832521915,\n",
       "  0.003999502398073673,\n",
       "  0.003992040641605854,\n",
       "  0.003986789379268885,\n",
       "  0.0039770333096385,\n",
       "  0.003970697522163391,\n",
       "  0.003960785456001759,\n",
       "  0.003960215486586094,\n",
       "  0.003959140740334988,\n",
       "  0.0039382027462124825,\n",
       "  0.003943729214370251,\n",
       "  0.003939148038625717,\n",
       "  0.003931405954062939,\n",
       "  0.003929148428142071,\n",
       "  0.003928039688616991,\n",
       "  0.003926097881048918,\n",
       "  0.003912018612027168,\n",
       "  0.003917715046554804,\n",
       "  0.003906001802533865,\n",
       "  0.003900995012372732,\n",
       "  0.003911192994564772,\n",
       "  0.0038942915853112936,\n",
       "  0.003891501110047102,\n",
       "  0.0038872165605425835,\n",
       "  0.0038816409651190042,\n",
       "  0.0038840703200548887,\n",
       "  0.00387743697501719,\n",
       "  0.0038783052004873753,\n",
       "  0.0038690196815878153,\n",
       "  0.0038680890575051308,\n",
       "  0.00386582943610847,\n",
       "  0.0038637283723801374,\n",
       "  0.003858278039842844,\n",
       "  0.003867669962346554,\n",
       "  0.0038544812705367804,\n",
       "  0.0038406639359891415,\n",
       "  0.0038575793150812387,\n",
       "  0.0038451170548796654,\n",
       "  0.003847758984193206,\n",
       "  0.0038414134178310633,\n",
       "  0.0038292829412966967,\n",
       "  0.003832671558484435,\n",
       "  0.0038282463792711496,\n",
       "  0.0038225387688726187,\n",
       "  0.0038200877606868744,\n",
       "  0.0038171943742781878,\n",
       "  0.0038117594085633755,\n",
       "  0.003818412544205785,\n",
       "  0.003814887721091509,\n",
       "  0.003804263425990939,\n",
       "  0.0038001611828804016,\n",
       "  0.00379912625066936,\n",
       "  0.0038054410833865404,\n",
       "  0.0038043842650949955,\n",
       "  0.0037983201909810305,\n",
       "  0.003788046073168516,\n",
       "  0.003797003300860524,\n",
       "  0.0037936565931886435,\n",
       "  0.003790045389905572,\n",
       "  0.0037834930699318647,\n",
       "  0.003777325851842761,\n",
       "  0.003784904256463051,\n",
       "  0.0037758813705295324,\n",
       "  0.0037813056260347366,\n",
       "  0.0037724189460277557,\n",
       "  0.003787863301113248,\n",
       "  0.003768674796447158,\n",
       "  0.0037696321960538626,\n",
       "  0.003766067326068878,\n",
       "  0.0037645967677235603,\n",
       "  0.0037596351467072964,\n",
       "  0.003759645391255617,\n",
       "  0.003763236105442047,\n",
       "  0.003760111751034856,\n",
       "  0.003756078891456127,\n",
       "  0.0037491684779524803,\n",
       "  0.003754511009901762,\n",
       "  0.0037547252140939236,\n",
       "  0.0037462019827216864,\n",
       "  0.0037511177361011505,\n",
       "  0.003743931883946061,\n",
       "  0.0037356463726609945,\n",
       "  0.0037355241365730762,\n",
       "  0.0037463242188096046,\n",
       "  0.003741996828466654,\n",
       "  0.0037361897993832827,\n",
       "  0.003741222433745861,\n",
       "  0.0037372794467955828,\n",
       "  0.0037323280703276396,\n",
       "  0.0037327499594539404,\n",
       "  0.0037309990730136633,\n",
       "  0.0037265485152602196,\n",
       "  0.003722281428053975,\n",
       "  0.003722695866599679,\n",
       "  0.003722117282450199,\n",
       "  0.003719307715073228,\n",
       "  0.003711442230269313,\n",
       "  0.0037156441248953342,\n",
       "  0.0037208402063697577,\n",
       "  0.0037120708730071783,\n",
       "  0.003713612211868167,\n",
       "  0.003707116236910224,\n",
       "  0.003705129958689213,\n",
       "  0.003710922785103321,\n",
       "  0.003706130897626281,\n",
       "  0.0037094971630722284,\n",
       "  0.003696238389238715,\n",
       "  0.003695321036502719,\n",
       "  0.003702425630763173,\n",
       "  0.0036988514475524426,\n",
       "  0.003692025551572442,\n",
       "  0.0036841630935668945,\n",
       "  0.0036871752236038446,\n",
       "  0.003696850501000881,\n",
       "  0.0036888609174638987,\n",
       "  0.0036888804752379656,\n",
       "  0.003687604097649455,\n",
       "  0.0036846748553216457,\n",
       "  0.003674877341836691,\n",
       "  0.0036826101131737232,\n",
       "  0.003675915068015456,\n",
       "  0.0036910520866513252,\n",
       "  0.003675331361591816,\n",
       "  0.0036736929323524237,\n",
       "  0.003677082946524024],\n",
       " 'accuracy': [0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell testen/anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57971014, 0.04162752, 0.22222222],\n",
       "       [0.34782609, 0.0191638 , 0.44444444],\n",
       "       [0.97101449, 0.04981187, 0.77777778],\n",
       "       [0.75362319, 0.0476758 , 0.44444444],\n",
       "       [0.96376812, 0.00879665, 0.66666667],\n",
       "       [0.76086957, 0.03311599, 0.44444444],\n",
       "       [0.71014493, 0.05073734, 0.55555556],\n",
       "       [0.71014493, 0.00428615, 0.33333333],\n",
       "       [0.95652174, 0.0505644 , 0.66666667],\n",
       "       [0.39130435, 0.02196826, 0.44444444]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0365141 ],\n",
       "       [0.10457019],\n",
       "       [0.3325172 ],\n",
       "       [0.14445186],\n",
       "       [0.25158042]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dollar = scaler_output.inverse_transform( preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_dollar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61193.8 ],\n",
       "       [110201.  ],\n",
       "       [274345.62],\n",
       "       [138919.78],\n",
       "       [216063.06],\n",
       "       [135997.58],\n",
       "       [175512.8 ],\n",
       "       [ 93826.82],\n",
       "       [229266.58],\n",
       "       [113059.67]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_dollar[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dollar = scaler_output.inverse_transform( y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dollar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 82000.],\n",
       "       [ 86000.],\n",
       "       [232000.],\n",
       "       [136905.],\n",
       "       [181000.],\n",
       "       [149900.],\n",
       "       [163500.],\n",
       "       [ 88000.],\n",
       "       [240000.],\n",
       "       [102000.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dollar[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61193.8] vs [82000.] --> Fehler: [20806.19921875]\n",
      "[110201.] vs [86000.] --> Fehler: [-24201.]\n",
      "[274345.62] vs [232000.] --> Fehler: [-42345.625]\n",
      "[138919.78] vs [136905.] --> Fehler: [-2014.78125]\n",
      "[216063.06] vs [181000.] --> Fehler: [-35063.0625]\n",
      "[135997.58] vs [149900.] --> Fehler: [13902.421875]\n",
      "[175512.8] vs [163500.] --> Fehler: [-12012.796875]\n",
      "[93826.82] vs [88000.] --> Fehler: [-5826.8203125]\n",
      "[229266.58] vs [240000.] --> Fehler: [10733.421875]\n",
      "[113059.67] vs [102000.] --> Fehler: [-11059.671875]\n",
      "[139271.42] vs [135000.] --> Fehler: [-4271.421875]\n",
      "[131838.03] vs [100000.] --> Fehler: [-31838.03125]\n",
      "[157302.14] vs [165000.] --> Fehler: [7697.859375]\n",
      "[112899.3] vs [85000.] --> Fehler: [-27899.296875]\n",
      "[166651.28] vs [119200.] --> Fehler: [-47451.28125]\n",
      "[267702.62] vs [227000.] --> Fehler: [-40702.625]\n",
      "[226804.8] vs [203000.] --> Fehler: [-23804.796875]\n",
      "[258720.14] vs [187500.] --> Fehler: [-71220.140625]\n",
      "[223855.89] vs [160000.] --> Fehler: [-63855.890625]\n",
      "[215809.] vs [213490.] --> Fehler: [-2319.]\n",
      "[120638.74] vs [176000.] --> Fehler: [55361.2578125]\n",
      "[222670.7] vs [194000.] --> Fehler: [-28670.703125]\n",
      "[118230.94] vs [87000.] --> Fehler: [-31230.9375]\n",
      "[216063.06] vs [191000.] --> Fehler: [-25063.0625]\n",
      "[270444.22] vs [287000.] --> Fehler: [16555.78125]\n",
      "[132400.38] vs [112500.] --> Fehler: [-19900.375]\n",
      "[128650.48] vs [167500.] --> Fehler: [38849.5234375]\n",
      "[270942.5] vs [293077.] --> Fehler: [22134.5]\n",
      "[121601.42] vs [105000.] --> Fehler: [-16601.421875]\n",
      "[162394.3] vs [118000.] --> Fehler: [-44394.296875]\n",
      "[112157.92] vs [160000.] --> Fehler: [47842.078125]\n",
      "[204489.34] vs [197000.] --> Fehler: [-7489.34375]\n",
      "[274856.4] vs [310000.] --> Fehler: [35143.59375]\n",
      "[222713.55] vs [230000.] --> Fehler: [7286.453125]\n",
      "[116966.16] vs [119750.] --> Fehler: [2783.84375]\n",
      "[96267.66] vs [84000.] --> Fehler: [-12267.65625]\n",
      "[320854.53] vs [315500.] --> Fehler: [-5354.53125]\n",
      "[268965.03] vs [287000.] --> Fehler: [18034.96875]\n",
      "[92966.63] vs [97000.] --> Fehler: [4033.3671875]\n",
      "[92896.22] vs [80000.] --> Fehler: [-12896.21875]\n",
      "[132274.3] vs [155000.] --> Fehler: [22725.703125]\n",
      "[169465.69] vs [173000.] --> Fehler: [3534.3125]\n",
      "[181032.28] vs [196000.] --> Fehler: [14967.71875]\n",
      "[225182.03] vs [262280.] --> Fehler: [37097.96875]\n",
      "[263349.88] vs [278000.] --> Fehler: [14650.125]\n",
      "[67025.195] vs [139600.] --> Fehler: [72574.8046875]\n",
      "[325081.84] vs [556581.] --> Fehler: [231499.15625]\n",
      "[148227.33] vs [145000.] --> Fehler: [-3227.328125]\n",
      "[144325.77] vs [115000.] --> Fehler: [-29325.765625]\n",
      "[88591.37] vs [84900.] --> Fehler: [-3691.3671875]\n",
      "[225315.27] vs [176485.] --> Fehler: [-48830.265625]\n",
      "[228629.17] vs [200141.] --> Fehler: [-28488.171875]\n",
      "[169237.1] vs [165000.] --> Fehler: [-4237.09375]\n",
      "[126578.38] vs [144500.] --> Fehler: [17921.6171875]\n",
      "[272421.16] vs [255000.] --> Fehler: [-17421.15625]\n",
      "[176243.88] vs [180000.] --> Fehler: [3756.125]\n",
      "[221879.23] vs [185850.] --> Fehler: [-36029.234375]\n",
      "[254139.02] vs [248000.] --> Fehler: [-6139.015625]\n",
      "[318724.34] vs [335000.] --> Fehler: [16275.65625]\n",
      "[160159.73] vs [220000.] --> Fehler: [59840.265625]\n",
      "[262472.53] vs [213500.] --> Fehler: [-48972.53125]\n",
      "[59914.156] vs [81000.] --> Fehler: [21085.84375]\n",
      "[123563.74] vs [90000.] --> Fehler: [-33563.7421875]\n",
      "[150950.64] vs [110500.] --> Fehler: [-40450.640625]\n",
      "[133692.56] vs [154000.] --> Fehler: [20307.4375]\n",
      "[230001.88] vs [328000.] --> Fehler: [97998.125]\n",
      "[177200.62] vs [178000.] --> Fehler: [799.375]\n",
      "[169635.06] vs [167900.] --> Fehler: [-1735.0625]\n",
      "[165629.] vs [151400.] --> Fehler: [-14229.]\n",
      "[124805.16] vs [135000.] --> Fehler: [10194.84375]\n",
      "[128179.56] vs [135000.] --> Fehler: [6820.4375]\n",
      "[174079.1] vs [154000.] --> Fehler: [-20079.09375]\n",
      "[121906.26] vs [91500.] --> Fehler: [-30406.2578125]\n",
      "[171793.27] vs [159500.] --> Fehler: [-12293.265625]\n",
      "[224399.05] vs [194000.] --> Fehler: [-30399.046875]\n",
      "[207938.23] vs [219500.] --> Fehler: [11561.765625]\n",
      "[121411.625] vs [170000.] --> Fehler: [48588.375]\n",
      "[140870.25] vs [138800.] --> Fehler: [-2070.25]\n",
      "[179121.89] vs [155900.] --> Fehler: [-23221.890625]\n",
      "[147686.66] vs [126000.] --> Fehler: [-21686.65625]\n",
      "[174174.73] vs [145000.] --> Fehler: [-29174.734375]\n",
      "[127940.445] vs [133000.] --> Fehler: [5059.5546875]\n",
      "[223651.34] vs [192000.] --> Fehler: [-31651.34375]\n",
      "[168287.9] vs [160000.] --> Fehler: [-8287.90625]\n",
      "[185005.2] vs [187500.] --> Fehler: [2494.796875]\n",
      "[178327.61] vs [147000.] --> Fehler: [-31327.609375]\n",
      "[95419.836] vs [83500.] --> Fehler: [-11919.8359375]\n",
      "[272007.7] vs [252000.] --> Fehler: [-20007.6875]\n",
      "[214811.64] vs [137500.] --> Fehler: [-77311.640625]\n",
      "[261099.66] vs [197000.] --> Fehler: [-64099.65625]\n",
      "[58132.066] vs [92900.] --> Fehler: [34767.93359375]\n",
      "[212970.4] vs [160000.] --> Fehler: [-52970.40625]\n",
      "[154310.6] vs [136500.] --> Fehler: [-17810.59375]\n",
      "[131120.38] vs [146000.] --> Fehler: [14879.625]\n",
      "[122831.89] vs [129000.] --> Fehler: [6168.109375]\n",
      "[186714.05] vs [176432.] --> Fehler: [-10282.046875]\n",
      "[152396.47] vs [127000.] --> Fehler: [-25396.46875]\n",
      "[256246.48] vs [170000.] --> Fehler: [-86246.484375]\n",
      "[78724.83] vs [128000.] --> Fehler: [49275.171875]\n",
      "[221737.17] vs [157000.] --> Fehler: [-64737.171875]\n",
      "[11254.4] vs [60000.] --> Fehler: [48745.59960938]\n",
      "[134425.25] vs [119500.] --> Fehler: [-14925.25]\n",
      "[125779.42] vs [135000.] --> Fehler: [9220.578125]\n",
      "[166577.39] vs [159500.] --> Fehler: [-7077.390625]\n",
      "[124690.56] vs [106000.] --> Fehler: [-18690.5625]\n",
      "[271681.72] vs [325000.] --> Fehler: [53318.28125]\n",
      "[222954.72] vs [179900.] --> Fehler: [-43054.71875]\n",
      "[246779.84] vs [274725.] --> Fehler: [27945.15625]\n",
      "[180704.17] vs [181000.] --> Fehler: [295.828125]\n",
      "[272932.53] vs [280000.] --> Fehler: [7067.46875]\n",
      "[178285.42] vs [188000.] --> Fehler: [9714.578125]\n",
      "[218901.53] vs [205000.] --> Fehler: [-13901.53125]\n",
      "[124753.4] vs [129900.] --> Fehler: [5146.6015625]\n",
      "[125494.555] vs [134500.] --> Fehler: [9005.4453125]\n",
      "[121480.49] vs [117000.] --> Fehler: [-4480.4921875]\n",
      "[274797.44] vs [318000.] --> Fehler: [43202.5625]\n",
      "[266985.2] vs [184100.] --> Fehler: [-82885.1875]\n",
      "[132654.05] vs [130000.] --> Fehler: [-2654.046875]\n",
      "[133531.72] vs [140000.] --> Fehler: [6468.28125]\n",
      "[126094.875] vs [133700.] --> Fehler: [7605.125]\n",
      "[154204.7] vs [118400.] --> Fehler: [-35804.703125]\n",
      "[226436.05] vs [212900.] --> Fehler: [-13536.046875]\n",
      "[92543.] vs [112000.] --> Fehler: [19457.]\n",
      "[123794.516] vs [118000.] --> Fehler: [-5794.515625]\n",
      "[221622.02] vs [163900.] --> Fehler: [-57722.015625]\n",
      "[93593.67] vs [115000.] --> Fehler: [21406.328125]\n",
      "[217336.39] vs [174000.] --> Fehler: [-43336.390625]\n",
      "[233007.36] vs [259000.] --> Fehler: [25992.640625]\n",
      "[228775.75] vs [215000.] --> Fehler: [-13775.75]\n",
      "[136618.05] vs [140000.] --> Fehler: [3381.953125]\n",
      "[78241.05] vs [135000.] --> Fehler: [56758.953125]\n",
      "[147931.3] vs [93500.] --> Fehler: [-54431.296875]\n",
      "[155701.61] vs [117500.] --> Fehler: [-38201.609375]\n",
      "[268292.1] vs [239500.] --> Fehler: [-28792.09375]\n",
      "[180185.67] vs [169000.] --> Fehler: [-11185.671875]\n",
      "[151275.17] vs [102000.] --> Fehler: [-49275.171875]\n",
      "[164280.05] vs [119000.] --> Fehler: [-45280.046875]\n",
      "[112949.13] vs [94000.] --> Fehler: [-18949.1328125]\n",
      "[174224.58] vs [196000.] --> Fehler: [21775.421875]\n",
      "[114509.21] vs [144000.] --> Fehler: [29490.7890625]\n",
      "[122763.8] vs [139000.] --> Fehler: [16236.203125]\n",
      "[137543.95] vs [197500.] --> Fehler: [59956.046875]\n",
      "[271351.88] vs [424870.] --> Fehler: [153518.125]\n",
      "[127892.35] vs [80000.] --> Fehler: [-47892.3515625]\n",
      "[87544.14] vs [80000.] --> Fehler: [-7544.140625]\n",
      "[114116.41] vs [149000.] --> Fehler: [34883.59375]\n",
      "[178398.67] vs [180000.] --> Fehler: [1601.328125]\n",
      "[206562.34] vs [174500.] --> Fehler: [-32062.34375]\n",
      "[189436.42] vs [116900.] --> Fehler: [-72536.421875]\n",
      "[193993.03] vs [143000.] --> Fehler: [-50993.03125]\n",
      "[162419.34] vs [124000.] --> Fehler: [-38419.34375]\n",
      "[138821.06] vs [149900.] --> Fehler: [11078.9375]\n",
      "[173352.] vs [230000.] --> Fehler: [56648.]\n",
      "[151426.75] vs [120500.] --> Fehler: [-30926.75]\n",
      "[219785.97] vs [201800.] --> Fehler: [-17985.96875]\n",
      "[138077.75] vs [218000.] --> Fehler: [79922.25]\n",
      "[131265.11] vs [179900.] --> Fehler: [48634.890625]\n",
      "[219315.67] vs [230000.] --> Fehler: [10684.328125]\n",
      "[274567.53] vs [235128.] --> Fehler: [-39439.53125]\n",
      "[172412.45] vs [185000.] --> Fehler: [12587.546875]\n",
      "[165032.9] vs [146000.] --> Fehler: [-19032.90625]\n",
      "[173629.58] vs [224000.] --> Fehler: [50370.421875]\n",
      "[131850.7] vs [129000.] --> Fehler: [-2850.703125]\n",
      "[106148.19] vs [108959.] --> Fehler: [2810.8125]\n",
      "[145599.45] vs [194000.] --> Fehler: [48400.546875]\n",
      "[226671.77] vs [233170.] --> Fehler: [6498.234375]\n",
      "[272637.25] vs [245350.] --> Fehler: [-27287.25]\n",
      "[184273.73] vs [173000.] --> Fehler: [-11273.734375]\n",
      "[165143.45] vs [235000.] --> Fehler: [69856.546875]\n",
      "[389639.7] vs [625000.] --> Fehler: [235360.3125]\n",
      "[174310.8] vs [171000.] --> Fehler: [-3310.796875]\n",
      "[166510.8] vs [163000.] --> Fehler: [-3510.796875]\n",
      "[217577.4] vs [171900.] --> Fehler: [-45677.40625]\n",
      "[134327.95] vs [200500.] --> Fehler: [66172.046875]\n",
      "[168572.05] vs [239000.] --> Fehler: [70427.953125]\n",
      "[268791.25] vs [285000.] --> Fehler: [16208.75]\n",
      "[138976.39] vs [119500.] --> Fehler: [-19476.390625]\n",
      "[148192.31] vs [115000.] --> Fehler: [-33192.3125]\n",
      "[113277.87] vs [154900.] --> Fehler: [41622.1328125]\n",
      "[125144.19] vs [93000.] --> Fehler: [-32144.1875]\n",
      "[224176.56] vs [250000.] --> Fehler: [25823.4375]\n",
      "[265291.06] vs [392500.] --> Fehler: [127208.9375]\n",
      "[364201.22] vs [745000.] --> Fehler: [380798.78125]\n",
      "[117265.65] vs [120000.] --> Fehler: [2734.3515625]\n",
      "[165039.55] vs [186700.] --> Fehler: [21660.453125]\n",
      "[116433.83] vs [104900.] --> Fehler: [-11533.828125]\n",
      "[36927.258] vs [95000.] --> Fehler: [58072.7421875]\n",
      "[271989.8] vs [262000.] --> Fehler: [-9989.8125]\n",
      "[223930.92] vs [195000.] --> Fehler: [-28930.921875]\n",
      "[220995.92] vs [189000.] --> Fehler: [-31995.921875]\n",
      "[124812.67] vs [168000.] --> Fehler: [43187.328125]\n",
      "[258534.53] vs [174000.] --> Fehler: [-84534.53125]\n",
      "[116662.625] vs [125000.] --> Fehler: [8337.375]\n",
      "[175888.1] vs [165000.] --> Fehler: [-10888.09375]\n",
      "[170906.98] vs [158000.] --> Fehler: [-12906.984375]\n",
      "[184357.47] vs [176000.] --> Fehler: [-8357.46875]\n",
      "[232751.14] vs [219210.] --> Fehler: [-13541.140625]\n",
      "[193630.34] vs [144000.] --> Fehler: [-49630.34375]\n",
      "[223919.42] vs [178000.] --> Fehler: [-45919.421875]\n",
      "[100203.54] vs [148000.] --> Fehler: [47796.4609375]\n",
      "[102539.93] vs [116050.] --> Fehler: [13510.0703125]\n",
      "[225094.88] vs [197900.] --> Fehler: [-27194.875]\n",
      "[112634.77] vs [117000.] --> Fehler: [4365.2265625]\n",
      "[224636.89] vs [213000.] --> Fehler: [-11636.890625]\n",
      "[136694.1] vs [153500.] --> Fehler: [16805.90625]\n",
      "[229457.78] vs [271900.] --> Fehler: [42442.21875]\n",
      "[99501.04] vs [107000.] --> Fehler: [7498.9609375]\n",
      "[184267.92] vs [200000.] --> Fehler: [15732.078125]\n",
      "[127711.14] vs [140000.] --> Fehler: [12288.859375]\n",
      "[271678.] vs [290000.] --> Fehler: [18322.]\n",
      "[181154.97] vs [189000.] --> Fehler: [7845.03125]\n",
      "[269703.84] vs [164000.] --> Fehler: [-105703.84375]\n",
      "[84503.82] vs [113000.] --> Fehler: [28496.1796875]\n",
      "[100388.22] vs [145000.] --> Fehler: [44611.78125]\n",
      "[130656.83] vs [134500.] --> Fehler: [3843.171875]\n",
      "[129005.54] vs [125000.] --> Fehler: [-4005.5390625]\n",
      "[173300.78] vs [112000.] --> Fehler: [-61300.78125]\n",
      "[270138.88] vs [229456.] --> Fehler: [-40682.875]\n",
      "[83829.24] vs [80500.] --> Fehler: [-3329.2421875]\n",
      "[162010.69] vs [91500.] --> Fehler: [-70510.6875]\n",
      "[128848.875] vs [115000.] --> Fehler: [-13848.875]\n",
      "[131281.9] vs [134000.] --> Fehler: [2718.09375]\n",
      "[165232.11] vs [143000.] --> Fehler: [-22232.109375]\n",
      "[126920.555] vs [137900.] --> Fehler: [10979.4453125]\n",
      "[234245.14] vs [184000.] --> Fehler: [-50245.140625]\n",
      "[168528.4] vs [145000.] --> Fehler: [-23528.40625]\n",
      "[194084.81] vs [214000.] --> Fehler: [19915.1875]\n",
      "[130688.52] vs [147000.] --> Fehler: [16311.4765625]\n",
      "[314908.5] vs [367294.] --> Fehler: [52385.5]\n",
      "[126915.36] vs [127000.] --> Fehler: [84.640625]\n",
      "[148082.22] vs [190000.] --> Fehler: [41917.78125]\n",
      "[127668.93] vs [132500.] --> Fehler: [4831.0703125]\n",
      "[97895.43] vs [101800.] --> Fehler: [3904.5703125]\n",
      "[131836.92] vs [142000.] --> Fehler: [10163.078125]\n",
      "[114658.87] vs [130000.] --> Fehler: [15341.1328125]\n",
      "[120895.41] vs [138887.] --> Fehler: [17991.59375]\n",
      "[214711.53] vs [175500.] --> Fehler: [-39211.53125]\n",
      "[229685.75] vs [195000.] --> Fehler: [-34685.75]\n",
      "[190690.17] vs [142500.] --> Fehler: [-48190.171875]\n",
      "[269957.16] vs [265900.] --> Fehler: [-4057.15625]\n",
      "[223022.6] vs [224900.] --> Fehler: [1877.40625]\n",
      "[226607.88] vs [248328.] --> Fehler: [21720.125]\n",
      "[218397.61] vs [170000.] --> Fehler: [-48397.609375]\n",
      "[366799.4] vs [465000.] --> Fehler: [98200.59375]\n",
      "[239452.67] vs [230000.] --> Fehler: [-9452.671875]\n",
      "[179488.66] vs [178000.] --> Fehler: [-1488.65625]\n",
      "[223491.88] vs [186500.] --> Fehler: [-36991.875]\n",
      "[177717.92] vs [169900.] --> Fehler: [-7817.921875]\n",
      "[156162.64] vs [129500.] --> Fehler: [-26662.640625]\n",
      "[122192.91] vs [119000.] --> Fehler: [-3192.90625]\n",
      "[217036.73] vs [244000.] --> Fehler: [26963.265625]\n",
      "[215475.] vs [171750.] --> Fehler: [-43725.]\n",
      "[133211.44] vs [130000.] --> Fehler: [-3211.4375]\n",
      "[228836.77] vs [294000.] --> Fehler: [65163.234375]\n",
      "[221178.53] vs [165400.] --> Fehler: [-55778.53125]\n",
      "[152048.72] vs [127500.] --> Fehler: [-24548.71875]\n",
      "[274758.12] vs [301500.] --> Fehler: [26741.875]\n",
      "[109214.86] vs [99900.] --> Fehler: [-9314.859375]\n",
      "[225689.06] vs [190000.] --> Fehler: [-35689.0625]\n",
      "[171462.92] vs [151000.] --> Fehler: [-20462.921875]\n",
      "[203994.48] vs [181000.] --> Fehler: [-22994.484375]\n",
      "[127490.52] vs [128900.] --> Fehler: [1409.4765625]\n",
      "[95975.2] vs [161500.] --> Fehler: [65524.796875]\n",
      "[161565.22] vs [180500.] --> Fehler: [18934.78125]\n",
      "[174891.39] vs [181000.] --> Fehler: [6108.609375]\n",
      "[215337.47] vs [183900.] --> Fehler: [-31437.46875]\n",
      "[195421.27] vs [122000.] --> Fehler: [-73421.265625]\n",
      "[321588.9] vs [378500.] --> Fehler: [56911.09375]\n",
      "[245457.95] vs [381000.] --> Fehler: [135542.046875]\n",
      "[130434.69] vs [144000.] --> Fehler: [13565.3125]\n",
      "[239943.75] vs [260000.] --> Fehler: [20056.25]\n",
      "[170247.14] vs [185750.] --> Fehler: [15502.859375]\n",
      "[136279.1] vs [137000.] --> Fehler: [720.90625]\n",
      "[170473.61] vs [177000.] --> Fehler: [6526.390625]\n",
      "[110278.23] vs [139000.] --> Fehler: [28721.7734375]\n",
      "[126599.78] vs [137000.] --> Fehler: [10400.21875]\n",
      "[177445.5] vs [162000.] --> Fehler: [-15445.5]\n",
      "[183156.12] vs [197900.] --> Fehler: [14743.875]\n",
      "[269574.7] vs [237000.] --> Fehler: [-32574.6875]\n",
      "[75680.86] vs [68400.] --> Fehler: [-7280.859375]\n",
      "[225242.94] vs [227000.] --> Fehler: [1757.0625]\n",
      "[219486.08] vs [180000.] --> Fehler: [-39486.078125]\n",
      "[136475.] vs [150500.] --> Fehler: [14025.]\n",
      "[171715.45] vs [139000.] --> Fehler: [-32715.453125]\n",
      "[156198.48] vs [169000.] --> Fehler: [12801.515625]\n",
      "[155428.61] vs [132500.] --> Fehler: [-22928.609375]\n",
      "[169312.34] vs [143000.] --> Fehler: [-26312.34375]\n",
      "[167532.8] vs [190000.] --> Fehler: [22467.203125]\n",
      "[263331.25] vs [278000.] --> Fehler: [14668.75]\n",
      "[273005.06] vs [281000.] --> Fehler: [7994.9375]\n",
      "[136462.8] vs [180500.] --> Fehler: [44037.203125]\n",
      "[125176.59] vs [119500.] --> Fehler: [-5676.59375]\n",
      "[114597.3] vs [107500.] --> Fehler: [-7097.296875]\n",
      "[218388.4] vs [162900.] --> Fehler: [-55488.40625]\n",
      "[125324.305] vs [115000.] --> Fehler: [-10324.3046875]\n",
      "[131461.89] vs [138500.] --> Fehler: [7038.109375]\n",
      "[129387.97] vs [155000.] --> Fehler: [25612.03125]\n",
      "[178908.81] vs [140000.] --> Fehler: [-38908.8125]\n",
      "[426742.47] vs [160000.] --> Fehler: [-266742.46875]\n",
      "[126019.94] vs [154000.] --> Fehler: [27980.0625]\n",
      "[225898.39] vs [225000.] --> Fehler: [-898.390625]\n",
      "[158480.48] vs [177500.] --> Fehler: [19019.515625]\n",
      "[268471.53] vs [290000.] --> Fehler: [21528.46875]\n",
      "[224338.] vs [232000.] --> Fehler: [7662.]\n",
      "[216075.6] vs [130000.] --> Fehler: [-86075.59375]\n",
      "[321008.38] vs [325000.] --> Fehler: [3991.625]\n",
      "[221733.48] vs [202500.] --> Fehler: [-19233.484375]\n",
      "[146877.92] vs [138000.] --> Fehler: [-8877.921875]\n",
      "[127417.41] vs [147000.] --> Fehler: [19582.59375]\n",
      "[175490.64] vs [179200.] --> Fehler: [3709.359375]\n",
      "[223368.39] vs [335000.] --> Fehler: [111631.609375]\n",
      "[224527.38] vs [203000.] --> Fehler: [-21527.375]\n",
      "[266560.06] vs [302000.] --> Fehler: [35439.9375]\n",
      "[321598.44] vs [333168.] --> Fehler: [11569.5625]\n",
      "[90255.04] vs [119000.] --> Fehler: [28744.9609375]\n",
      "[173491.05] vs [206900.] --> Fehler: [33408.953125]\n",
      "[272263.] vs [295493.] --> Fehler: [23230.]\n",
      "[217897.5] vs [208900.] --> Fehler: [-8997.5]\n",
      "[277292.62] vs [275000.] --> Fehler: [-2292.625]\n",
      "[92623.99] vs [111000.] --> Fehler: [18376.0078125]\n",
      "[165149.95] vs [156500.] --> Fehler: [-8649.953125]\n",
      "[55353.707] vs [72500.] --> Fehler: [17146.29296875]\n",
      "[223216.61] vs [190000.] --> Fehler: [-33216.609375]\n",
      "[79770.27] vs [82500.] --> Fehler: [2729.7265625]\n",
      "[271383.44] vs [147000.] --> Fehler: [-124383.4375]\n",
      "[71937.43] vs [55000.] --> Fehler: [-16937.4296875]\n",
      "[42409.836] vs [79000.] --> Fehler: [36590.1640625]\n",
      "[137250.19] vs [130500.] --> Fehler: [-6750.1875]\n",
      "[157208.1] vs [256000.] --> Fehler: [98791.90625]\n",
      "[223117.05] vs [176500.] --> Fehler: [-46617.046875]\n",
      "[271404.47] vs [227000.] --> Fehler: [-44404.46875]\n",
      "[138091.53] vs [132500.] --> Fehler: [-5591.53125]\n",
      "[82686.12] vs [100000.] --> Fehler: [17313.8828125]\n",
      "[117961.89] vs [125500.] --> Fehler: [7538.109375]\n",
      "[125112.96] vs [125000.] --> Fehler: [-112.9609375]\n",
      "[173991.75] vs [167900.] --> Fehler: [-6091.75]\n",
      "[134700.14] vs [135000.] --> Fehler: [299.859375]\n",
      "[78913.46] vs [52500.] --> Fehler: [-26413.4609375]\n",
      "[230763.64] vs [200000.] --> Fehler: [-30763.640625]\n",
      "[134632.28] vs [128500.] --> Fehler: [-6132.28125]\n",
      "[101870.11] vs [123000.] --> Fehler: [21129.890625]\n",
      "[190012.1] vs [155000.] --> Fehler: [-35012.09375]\n",
      "[269427.38] vs [228500.] --> Fehler: [-40927.375]\n",
      "[153456.44] vs [177000.] --> Fehler: [23543.5625]\n",
      "[228252.73] vs [155835.] --> Fehler: [-72417.734375]\n",
      "[74012.03] vs [108500.] --> Fehler: [34487.96875]\n",
      "[231594.05] vs [262500.] --> Fehler: [30905.953125]\n",
      "[279378.9] vs [283463.] --> Fehler: [4084.09375]\n",
      "[233882.77] vs [215000.] --> Fehler: [-18882.765625]\n",
      "[218120.97] vs [122000.] --> Fehler: [-96120.96875]\n",
      "[135798.] vs [200000.] --> Fehler: [64202.]\n",
      "[168145.48] vs [171000.] --> Fehler: [2854.515625]\n",
      "[154503.8] vs [134900.] --> Fehler: [-19603.796875]\n",
      "[275644.3] vs [410000.] --> Fehler: [134355.6875]\n",
      "[225487.55] vs [235000.] --> Fehler: [9512.453125]\n",
      "[215575.81] vs [170000.] --> Fehler: [-45575.8125]\n",
      "[131865.02] vs [110000.] --> Fehler: [-21865.015625]\n",
      "[137759.97] vs [149900.] --> Fehler: [12140.03125]\n",
      "[173810.53] vs [177500.] --> Fehler: [3689.46875]\n",
      "[325845.88] vs [315000.] --> Fehler: [-10845.875]\n",
      "[116057.06] vs [189000.] --> Fehler: [72942.9375]\n",
      "[235691.7] vs [260000.] --> Fehler: [24308.296875]\n",
      "[81410.98] vs [104900.] --> Fehler: [23489.0234375]\n",
      "[185686.86] vs [156932.] --> Fehler: [-28754.859375]\n",
      "[216060.02] vs [144152.] --> Fehler: [-71908.015625]\n",
      "[221255.38] vs [216000.] --> Fehler: [-5255.375]\n",
      "[223519.28] vs [193000.] --> Fehler: [-30519.28125]\n",
      "[129417.52] vs [127000.] --> Fehler: [-2417.5234375]\n",
      "[178460.67] vs [144000.] --> Fehler: [-34460.671875]\n",
      "[271580.53] vs [232000.] --> Fehler: [-39580.53125]\n",
      "[73344.49] vs [105000.] --> Fehler: [31655.5078125]\n",
      "[166098.73] vs [165500.] --> Fehler: [-598.734375]\n",
      "[224117.98] vs [274300.] --> Fehler: [50182.015625]\n",
      "[360311.94] vs [466500.] --> Fehler: [106188.0625]\n",
      "[227257.39] vs [250000.] --> Fehler: [22742.609375]\n",
      "[273153.28] vs [239000.] --> Fehler: [-34153.28125]\n",
      "[151982.5] vs [91000.] --> Fehler: [-60982.5]\n",
      "[123481.35] vs [117000.] --> Fehler: [-6481.3515625]\n",
      "[163118.7] vs [83000.] --> Fehler: [-80118.703125]\n",
      "[156242.78] vs [167500.] --> Fehler: [11257.21875]\n",
      "[38049.35] vs [58500.] --> Fehler: [20450.6484375]\n",
      "[176691.4] vs [237500.] --> Fehler: [60808.59375]\n",
      "[191485.7] vs [157000.] --> Fehler: [-34485.703125]\n",
      "[137287.66] vs [112000.] --> Fehler: [-25287.65625]\n",
      "[159584.89] vs [105000.] --> Fehler: [-54584.890625]\n",
      "[73940.04] vs [125500.] --> Fehler: [51559.9609375]\n",
      "[228999.4] vs [250000.] --> Fehler: [21000.59375]\n",
      "[154631.6] vs [136000.] --> Fehler: [-18631.59375]\n",
      "[323591.88] vs [377500.] --> Fehler: [53908.125]\n",
      "[156161.66] vs [131000.] --> Fehler: [-25161.65625]\n",
      "[223660.] vs [235000.] --> Fehler: [11340.]\n",
      "[131670.08] vs [124000.] --> Fehler: [-7670.078125]\n",
      "[130342.88] vs [123000.] --> Fehler: [-7342.8828125]\n",
      "[158029.27] vs [163000.] --> Fehler: [4970.734375]\n",
      "[217619.47] vs [246578.] --> Fehler: [28958.53125]\n",
      "[275150.28] vs [281213.] --> Fehler: [6062.71875]\n",
      "[191974.66] vs [160000.] --> Fehler: [-31974.65625]\n",
      "[111107.016] vs [137500.] --> Fehler: [26392.984375]\n",
      "[122192.91] vs [138000.] --> Fehler: [15807.09375]\n",
      "[151186.23] vs [137450.] --> Fehler: [-13736.234375]\n",
      "[151440.7] vs [120000.] --> Fehler: [-31440.703125]\n",
      "[182847.22] vs [193000.] --> Fehler: [10152.78125]\n",
      "[221702.83] vs [193879.] --> Fehler: [-27823.828125]\n",
      "[279563.16] vs [282922.] --> Fehler: [3358.84375]\n",
      "[40892.047] vs [105000.] --> Fehler: [64107.953125]\n",
      "[261630.72] vs [275000.] --> Fehler: [13369.28125]\n",
      "[133294.33] vs [133000.] --> Fehler: [-294.328125]\n",
      "[141741.6] vs [112000.] --> Fehler: [-29741.59375]\n",
      "[72915.445] vs [125500.] --> Fehler: [52584.5546875]\n",
      "[237310.39] vs [215000.] --> Fehler: [-22310.390625]\n",
      "[228926.86] vs [230000.] --> Fehler: [1073.140625]\n",
      "[164280.05] vs [140000.] --> Fehler: [-24280.046875]\n",
      "[85764.44] vs [90000.] --> Fehler: [4235.5625]\n",
      "[272638.9] vs [257000.] --> Fehler: [-15638.90625]\n",
      "[160586.1] vs [207000.] --> Fehler: [46413.90625]\n",
      "[217262.73] vs [175900.] --> Fehler: [-41362.734375]\n",
      "[82727.32] vs [122500.] --> Fehler: [39772.6796875]\n",
      "[279165.47] vs [340000.] --> Fehler: [60834.53125]\n",
      "[129995.91] vs [124000.] --> Fehler: [-5995.90625]\n",
      "[181612.77] vs [223000.] --> Fehler: [41387.234375]\n",
      "[174079.1] vs [179900.] --> Fehler: [5820.90625]\n",
      "[166940.84] vs [127500.] --> Fehler: [-39440.84375]\n",
      "[178460.67] vs [136500.] --> Fehler: [-41960.671875]\n",
      "[185594.62] vs [274970.] --> Fehler: [89375.375]\n",
      "[128136.64] vs [144000.] --> Fehler: [15863.359375]\n",
      "[169262.84] vs [142000.] --> Fehler: [-27262.84375]\n",
      "[224874.69] vs [271000.] --> Fehler: [46125.3125]\n",
      "[124930.89] vs [140000.] --> Fehler: [15069.109375]\n",
      "[118517.4] vs [119000.] --> Fehler: [482.6015625]\n",
      "[179383.69] vs [182900.] --> Fehler: [3516.3125]\n",
      "[171568.73] vs [192140.] --> Fehler: [20571.265625]\n",
      "[167758.33] vs [143750.] --> Fehler: [-24008.328125]\n",
      "[81316.39] vs [64500.] --> Fehler: [-16816.390625]\n",
      "[183496.34] vs [186500.] --> Fehler: [3003.65625]\n",
      "[146368.16] vs [160000.] --> Fehler: [13631.84375]\n",
      "[167018.81] vs [174000.] --> Fehler: [6981.1875]\n",
      "[102732.43] vs [120500.] --> Fehler: [17767.5703125]\n",
      "[275596.44] vs [394617.] --> Fehler: [119020.5625]\n",
      "[163855.25] vs [149700.] --> Fehler: [-14155.25]\n",
      "[221651.84] vs [197000.] --> Fehler: [-24651.84375]\n",
      "[158513.77] vs [191000.] --> Fehler: [32486.234375]\n",
      "[179111.53] vs [149300.] --> Fehler: [-29811.53125]\n",
      "[363101.2] vs [310000.] --> Fehler: [-53101.1875]\n",
      "[155100.75] vs [121000.] --> Fehler: [-34100.75]\n",
      "[223796.] vs [179600.] --> Fehler: [-44196.]\n",
      "[168513.9] vs [129000.] --> Fehler: [-39513.90625]\n",
      "[151134.52] vs [157900.] --> Fehler: [6765.484375]\n",
      "[268550.6] vs [240000.] --> Fehler: [-28550.59375]\n",
      "[78167.69] vs [112000.] --> Fehler: [33832.3125]\n",
      "[124110.984] vs [92000.] --> Fehler: [-32110.984375]\n",
      "[134835.81] vs [136000.] --> Fehler: [1164.1875]\n",
      "[270814.22] vs [287090.] --> Fehler: [16275.78125]\n",
      "[148329.7] vs [145000.] --> Fehler: [-3329.703125]\n",
      "[166232.92] vs [84500.] --> Fehler: [-81732.921875]\n",
      "[222293.12] vs [185000.] --> Fehler: [-37293.125]\n",
      "[179964.25] vs [175000.] --> Fehler: [-4964.25]\n",
      "[179542.44] vs [210000.] --> Fehler: [30457.5625]\n",
      "[201557.84] vs [266500.] --> Fehler: [64942.15625]\n",
      "[125334.84] vs [142125.] --> Fehler: [16790.15625]\n",
      "[131972.4] vs [147500.] --> Fehler: [15527.59375]\n",
      "Durchschnittlicher Fehler in $: [30215.84356743]\n"
     ]
    }
   ],
   "source": [
    "nr_tests = len(y_test)\n",
    "sum_errors = 0.0\n",
    "for i in range(0,nr_tests):\n",
    "    error = gt_dollar[i] - preds_dollar[i]\n",
    "    print(\"{0} vs {1} --> Fehler: {2}\"\n",
    "          .format(preds_dollar[i],\n",
    "                  gt_dollar[i],\n",
    "                  error ))\n",
    "    sum_errors += abs(error)\n",
    "print(\"Durchschnittlicher Fehler in $:\", sum_errors/nr_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname1 = \"hauspreis_schaetzer.h5\"\n",
    "model.save(fname1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "daten = [1, 2.345, \"djksjdks\", {\"DE\": \"Deutschland\", \"FR\" : \"Frankreich\"}]\n",
    "\n",
    "fname = \"ein_paar_daten.pkl\"\n",
    "fobj = open(fname, \"wb\")\n",
    "pickle.dump(daten, fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fobj = open(fname, \"rb\")\n",
    "daten2 = pickle.load(fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2.345, 'djksjdks', {'DE': 'Deutschland', 'FR': 'Frankreich'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daten2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fname2 = \"scaler_input.pkl\"\n",
    "fobj = open(fname2, \"wb\")\n",
    "pickle.dump(scaler_input, fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fname2 = \"scaler_output.pkl\"\n",
    "fobj = open(fname2, \"wb\")\n",
    "pickle.dump(scaler_output, fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell wiederherstellen und anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "new_model = keras.models.load_model(\"hauspreis_schaetzer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 3,601\n",
      "Trainable params: 3,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fobj = open(\"scaler_input.pkl\", \"rb\")\n",
    "scaler_input = pickle.load(fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.preprocessing._data.MinMaxScaler"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaler_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fobj = open(\"scaler_output.pkl\", \"rb\")\n",
    "scaler_output = pickle.load(fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.preprocessing._data.MinMaxScaler"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Baujahr: 2000\n",
      "Groesse: 5000\n",
      "Qualität: 5\n"
     ]
    }
   ],
   "source": [
    "baujahr = float(input(\"Baujahr:\"))\n",
    "groesse = float(input(\"Groesse:\"))\n",
    "qualitaet = float(input(\"Qualität:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000.0, 5000.0, 5.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baujahr, groesse, qualitaet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix = scaler_input.transform( [[baujahr, groesse, qualitaet]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92753623, 0.01729416, 0.44444444]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict( input_matrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15549377]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dollar = scaler_output.inverse_transform( pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[146871.06]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dollar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Inhalte",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

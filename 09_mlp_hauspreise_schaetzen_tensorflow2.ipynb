{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Jupyter-Notebook durchlaufen wir alle relevanten Schritte des Machine-Learnings:\n",
    "1. Daten einlesen\n",
    "2. Daten vorverarbeiten\n",
    "3. Trainings- und Testdaten vorbereiten\n",
    "4. Machine-Learning Modell definieren (hier: ein MLP)\n",
    "5. Modell trainieren\n",
    "6. Modell testen/anwenden\n",
    "7. Modell speichern/wiederherstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verwendeter Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen mit realen Daten arbeiten. Bei [Kaggle](https://www.kaggle.com/) können wir viele Datensätze finden. Diesen hier verwenden wir im Folgenden:\n",
    "\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Der Datensatz enthält in den Trainingsdaten 1460 Beispiele von Häusern, wobei deren Eigenschaften und deren jeweiliger tatsächlicher Verkaufspreis aufgeführt ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datensätze liegen often als .csv Dateien vor. Diese können mittels der Bibliothek Pandas einfach eingelesen werden.\n",
    "\n",
    "Wenn Pandas noch nicht installiert ist, kann diese Bibliothek mittels\n",
    "\n",
    "    conda install pandas\n",
    "\n",
    "unter der Anaconda Prompt installiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"daten/hausbeispiele.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal     208500  \n",
       "1         5   2007        WD         Normal     181500  \n",
       "2         9   2008        WD         Normal     223500  \n",
       "3         2   2006        WD        Abnorml     140000  \n",
       "4        12   2008        WD         Normal     250000  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1455      8   2007        WD         Normal     175000  \n",
       "1456      2   2010        WD         Normal     210000  \n",
       "1457      5   2010        WD         Normal     266500  \n",
       "1458      4   2010        WD         Normal     142125  \n",
       "1459      6   2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 81 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "\n",
       "[3 rows x 81 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spalten selektieren, Daten plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2003\n",
       "1       1976\n",
       "2       2001\n",
       "3       1915\n",
       "4       2000\n",
       "        ... \n",
       "1455    1999\n",
       "1456    1978\n",
       "1457    1941\n",
       "1458    1950\n",
       "1459    1965\n",
       "Name: YearBuilt, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"YearBuilt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500\n",
       "1       181500\n",
       "2       223500\n",
       "3       140000\n",
       "4       250000\n",
       "         ...  \n",
       "1455    175000\n",
       "1456    210000\n",
       "1457    266500\n",
       "1458    142125\n",
       "1459    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"YearBuilt\"]\n",
    "y = df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y, marker=\"+\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = df[[\"YearBuilt\", \"LotArea\"]].values\n",
    "#data_input = df[[\"YearBuilt\", \"LotArea\", \"OverallQual\"]].values\n",
    "\n",
    "nr_inputs = data_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2003,  8450],\n",
       "       [ 1976,  9600],\n",
       "       [ 2001, 11250],\n",
       "       ...,\n",
       "       [ 1941,  9042],\n",
       "       [ 1950,  9717],\n",
       "       [ 1965,  9937]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = df[\"SalePrice\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([208500, 181500, 223500, ..., 266500, 142125, 147500])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_input = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input_data = scaler_input.fit_transform(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94927536, 0.0334198 ],\n",
       "       [0.75362319, 0.03879502],\n",
       "       [0.93478261, 0.04650728],\n",
       "       ...,\n",
       "       [0.5       , 0.03618687],\n",
       "       [0.56521739, 0.03934189],\n",
       "       [0.67391304, 0.04037019]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaled_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_output = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_output_data = scaler_output.fit_transform(data_output.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24107763],\n",
       "       [0.20358284],\n",
       "       [0.26190807],\n",
       "       ...,\n",
       "       [0.321622  ],\n",
       "       [0.14890293],\n",
       "       [0.15636717]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings- und Testdaten definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaled_input_data[0:1000]\n",
    "y_train = scaled_output_data[0:1000]\n",
    "x_test  = scaled_input_data[1000:]\n",
    "y_test  = scaled_output_data[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94927536 0.0334198 ] --> [0.24107763]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0], \"-->\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(20,\n",
    "                             activation=\"relu\",\n",
    "                             input_shape=(nr_inputs,)))\n",
    "model.add(keras.layers.Dense(1,\n",
    "                             activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',               \n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 0.0426 - accuracy: 0.0010\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.0145 - accuracy: 0.0010\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0116 - accuracy: 0.0010\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.0111 - accuracy: 0.0010\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 0.0109 - accuracy: 0.0010\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 0.0109 - accuracy: 0.0010\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0108 - accuracy: 0.0010\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 0.0107 - accuracy: 0.0010\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.0107 - accuracy: 0.0010\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 0.0106 - accuracy: 0.0010\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.0105 - accuracy: 0.0010\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.0105 - accuracy: 0.0010\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0104 - accuracy: 0.0010\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.0104 - accuracy: 0.0010\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.0103 - accuracy: 0.0010\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 0.0102 - accuracy: 0.0010\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.0102 - accuracy: 0.0010\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.0101 - accuracy: 0.0010\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.0101 - accuracy: 0.0010\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 0s 72us/sample - loss: 0.0100 - accuracy: 0.0010\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 0s 106us/sample - loss: 0.0100 - accuracy: 0.0010\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.0100 - accuracy: 0.0010\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.0099 - accuracy: 0.0010\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.0099 - accuracy: 0.0010\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 0.0098 - accuracy: 0.0010\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 0s 110us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0097 - accuracy: 0.0010\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 0s 98us/sample - loss: 0.0096 - accuracy: 0.0010\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 0s 103us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 0s 101us/sample - loss: 0.0095 - accuracy: 0.0010\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 0s 182us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 0s 123us/sample - loss: 0.0094 - accuracy: 0.0010\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 0s 215us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 0s 113us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 0s 64us/sample - loss: 0.0093 - accuracy: 0.0010\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 0s 111us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 0s 109us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 0.0092 - accuracy: 0.0010\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 0s 205us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 0.0091 - accuracy: 0.0010\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.0090 - accuracy: 0.0010\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 0.0090 - accuracy: 0.0010\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 0s 175us/sample - loss: 0.0090 - accuracy: 0.0010\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 0s 163us/sample - loss: 0.0090 - accuracy: 0.0010\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 0.0090 - accuracy: 0.0010\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 0s 108us/sample - loss: 0.0089 - accuracy: 0.0010\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 0s 109us/sample - loss: 0.0089 - accuracy: 0.0010\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 0.0089 - accuracy: 0.0010\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.0089 - accuracy: 0.0010\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 0s 106us/sample - loss: 0.0089 - accuracy: 0.0010\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 0.0089 - accuracy: 0.0010\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 0.0089 - accuracy: 0.0010\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 0.0088 - accuracy: 0.0010\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 0s 98us/sample - loss: 0.0088 - accuracy: 0.0010\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.0088 - accuracy: 0.0010\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 0.0088 - accuracy: 0.0010\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 0s 182us/sample - loss: 0.0088 - accuracy: 0.0010\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 0s 134us/sample - loss: 0.0088 - accuracy: 0.0010\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 0s 92us/sample - loss: 0.0088 - accuracy: 0.0010\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 0s 110us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 0s 127us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 0s 114us/sample - loss: 0.0087 - accuracy: 0.0010\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 0s 125us/sample - loss: 0.0086 - accuracy: 0.0010\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 0.0086 - accuracy: 0.0010\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0086 - accuracy: 0.0010\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 0.0086 - accuracy: 0.0010\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 0s 152us/sample - loss: 0.0086 - accuracy: 0.0010\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 0s 207us/sample - loss: 0.0086 - accuracy: 0.0010\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 0.0086 - accuracy: 0.0010\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0086 - accuracy: 0.0010\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 0s 55us/sample - loss: 0.0085 - accuracy: 0.0010\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 0s 62us/sample - loss: 0.0085 - accuracy: 0.0010\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0085 - accuracy: 0.0010\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 0s 62us/sample - loss: 0.0085 - accuracy: 0.0010\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 0.0085 - accuracy: 0.0010\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 0.0085 - accuracy: 0.0010\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 0s 100us/sample - loss: 0.0084 - accuracy: 0.0010\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 0s 92us/sample - loss: 0.0084 - accuracy: 0.0010\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 0s 187us/sample - loss: 0.0084 - accuracy: 0.0010\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 0s 109us/sample - loss: 0.0084 - accuracy: 0.0010\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 0.0084 - accuracy: 0.0010\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 0s 127us/sample - loss: 0.0084 - accuracy: 0.0010\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.0084 - accuracy: 0.0010\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 0s 123us/sample - loss: 0.0084 - accuracy: 0.0010\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 0s 192us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 0s 108us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 0s 75us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.0083 - accuracy: 0.0010\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 0s 103us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 0s 114us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 0s 100us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.0082 - accuracy: 0.0010\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 0s 199us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 0s 178us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 0s 58us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 0s 152us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 0s 65us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 0s 191us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0081 - accuracy: 0.0010\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 0s 169us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 0s 113us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 0s 128us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 0s 176us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 0s 123us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 0s 194us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 0s 107us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 0s 200us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 0s 109us/sample - loss: 0.0080 - accuracy: 0.0010\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 0s 100us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 0s 202us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 0s 182us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 0s 137us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 0s 128us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 0s 169us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 0s 122us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 0s 111us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 0s 123us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 0s 145us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 0s 192us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 0s 138us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 0s 126us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 0s 72us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 0.0079 - accuracy: 0.0010\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 0s 104us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 0s 128us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 0s 106us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 0.0078 - accuracy: 0.0010\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 0.0078 - accuracy: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell testen/anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15908532],\n",
       "       [0.10476176],\n",
       "       [0.27181312],\n",
       "       [0.2071799 ],\n",
       "       [0.2566784 ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dollar = scaler_output.inverse_transform( preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_dollar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149457.34 ],\n",
       "       [110338.945],\n",
       "       [230632.62 ],\n",
       "       [184090.25 ],\n",
       "       [219734.11 ],\n",
       "       [182301.56 ],\n",
       "       [175578.23 ],\n",
       "       [164973.88 ],\n",
       "       [227734.1  ],\n",
       "       [117203.61 ]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_dollar[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dollar = scaler_output.inverse_transform( y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dollar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 82000.],\n",
       "       [ 86000.],\n",
       "       [232000.],\n",
       "       [136905.],\n",
       "       [181000.],\n",
       "       [149900.],\n",
       "       [163500.],\n",
       "       [ 88000.],\n",
       "       [240000.],\n",
       "       [102000.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dollar[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149457.34] vs [82000.] --> Fehler: [-67457.34375]\n",
      "[110338.945] vs [86000.] --> Fehler: [-24338.9453125]\n",
      "[230632.62] vs [232000.] --> Fehler: [1367.375]\n",
      "[184090.25] vs [136905.] --> Fehler: [-47185.25]\n",
      "[219734.11] vs [181000.] --> Fehler: [-38734.109375]\n",
      "[182301.56] vs [149900.] --> Fehler: [-32401.5625]\n",
      "[175578.23] vs [163500.] --> Fehler: [-12078.234375]\n",
      "[164973.88] vs [88000.] --> Fehler: [-76973.875]\n",
      "[227734.1] vs [240000.] --> Fehler: [12265.90625]\n",
      "[117203.61] vs [102000.] --> Fehler: [-15203.609375]\n",
      "[162800.31] vs [135000.] --> Fehler: [-27800.3125]\n",
      "[165416.22] vs [100000.] --> Fehler: [-65416.21875]\n",
      "[121407.31] vs [165000.] --> Fehler: [43592.6875]\n",
      "[103268.414] vs [85000.] --> Fehler: [-18268.4140625]\n",
      "[147781.1] vs [119200.] --> Fehler: [-28581.09375]\n",
      "[219161.36] vs [227000.] --> Fehler: [7838.640625]\n",
      "[215202.1] vs [203000.] --> Fehler: [-12202.09375]\n",
      "[190304.27] vs [187500.] --> Fehler: [-2804.265625]\n",
      "[206353.64] vs [160000.] --> Fehler: [-46353.640625]\n",
      "[219553.78] vs [213490.] --> Fehler: [-6063.78125]\n",
      "[223833.72] vs [176000.] --> Fehler: [-47833.71875]\n",
      "[225776.47] vs [194000.] --> Fehler: [-31776.46875]\n",
      "[126523.7] vs [87000.] --> Fehler: [-39523.703125]\n",
      "[219734.11] vs [191000.] --> Fehler: [-28734.109375]\n",
      "[188356.33] vs [287000.] --> Fehler: [98643.671875]\n",
      "[173894.84] vs [112500.] --> Fehler: [-61394.84375]\n",
      "[157180.23] vs [167500.] --> Fehler: [10319.765625]\n",
      "[229567.39] vs [293077.] --> Fehler: [63509.609375]\n",
      "[137474.64] vs [105000.] --> Fehler: [-32474.640625]\n",
      "[167471.19] vs [118000.] --> Fehler: [-49471.1875]\n",
      "[109012.34] vs [160000.] --> Fehler: [50987.65625]\n",
      "[126670.89] vs [197000.] --> Fehler: [70329.109375]\n",
      "[213432.84] vs [310000.] --> Fehler: [96567.15625]\n",
      "[220403.1] vs [230000.] --> Fehler: [9596.90625]\n",
      "[129535.92] vs [119750.] --> Fehler: [-9785.921875]\n",
      "[156417.28] vs [84000.] --> Fehler: [-72417.28125]\n",
      "[233171.89] vs [315500.] --> Fehler: [82328.109375]\n",
      "[220057.66] vs [287000.] --> Fehler: [66942.34375]\n",
      "[164244.02] vs [97000.] --> Fehler: [-67244.015625]\n",
      "[164184.27] vs [80000.] --> Fehler: [-84184.265625]\n",
      "[158953.77] vs [155000.] --> Fehler: [-3953.765625]\n",
      "[166209.77] vs [173000.] --> Fehler: [6790.234375]\n",
      "[222080.56] vs [196000.] --> Fehler: [-26080.5625]\n",
      "[205944.2] vs [262280.] --> Fehler: [56335.796875]\n",
      "[189738.67] vs [278000.] --> Fehler: [88261.328125]\n",
      "[157844.05] vs [139600.] --> Fehler: [-18244.046875]\n",
      "[233471.31] vs [556581.] --> Fehler: [323109.6875]\n",
      "[209316.9] vs [145000.] --> Fehler: [-64316.90625]\n",
      "[175380.81] vs [115000.] --> Fehler: [-60380.8125]\n",
      "[144924.7] vs [84900.] --> Fehler: [-60024.703125]\n",
      "[229005.05] vs [176485.] --> Fehler: [-52520.046875]\n",
      "[231333.36] vs [200141.] --> Fehler: [-31192.359375]\n",
      "[163534.27] vs [165000.] --> Fehler: [1465.734375]\n",
      "[151831.27] vs [144500.] --> Fehler: [-7331.265625]\n",
      "[223862.44] vs [255000.] --> Fehler: [31137.5625]\n",
      "[183941.94] vs [180000.] --> Fehler: [-3941.9375]\n",
      "[223863.6] vs [185850.] --> Fehler: [-38013.59375]\n",
      "[231419.78] vs [248000.] --> Fehler: [16580.21875]\n",
      "[233010.45] vs [335000.] --> Fehler: [101989.546875]\n",
      "[131365.6] vs [220000.] --> Fehler: [88634.40625]\n",
      "[215448.02] vs [213500.] --> Fehler: [-1948.015625]\n",
      "[144826.88] vs [81000.] --> Fehler: [-63826.875]\n",
      "[103378.14] vs [90000.] --> Fehler: [-13378.140625]\n",
      "[116215.6] vs [110500.] --> Fehler: [-5715.6015625]\n",
      "[168205.16] vs [154000.] --> Fehler: [-14205.15625]\n",
      "[217461.03] vs [328000.] --> Fehler: [110538.96875]\n",
      "[206279.34] vs [178000.] --> Fehler: [-28279.34375]\n",
      "[163811.7] vs [167900.] --> Fehler: [4088.296875]\n",
      "[171443.5] vs [151400.] --> Fehler: [-20043.5]\n",
      "[145547.38] vs [135000.] --> Fehler: [-10547.375]\n",
      "[153325.12] vs [135000.] --> Fehler: [-18325.125]\n",
      "[172022.42] vs [154000.] --> Fehler: [-18022.421875]\n",
      "[141414.1] vs [91500.] --> Fehler: [-49914.09375]\n",
      "[181837.39] vs [159500.] --> Fehler: [-22337.390625]\n",
      "[227003.6] vs [194000.] --> Fehler: [-33003.59375]\n",
      "[142157.45] vs [219500.] --> Fehler: [77342.546875]\n",
      "[134576.23] vs [170000.] --> Fehler: [35423.765625]\n",
      "[178007.16] vs [138800.] --> Fehler: [-39207.15625]\n",
      "[219535.97] vs [155900.] --> Fehler: [-63635.96875]\n",
      "[208815.4] vs [126000.] --> Fehler: [-82815.40625]\n",
      "[175923.64] vs [145000.] --> Fehler: [-30923.640625]\n",
      "[159865.03] vs [133000.] --> Fehler: [-26865.03125]\n",
      "[221068.9] vs [192000.] --> Fehler: [-29068.90625]\n",
      "[162787.33] vs [160000.] --> Fehler: [-2787.328125]\n",
      "[214891.9] vs [187500.] --> Fehler: [-27391.90625]\n",
      "[206058.77] vs [147000.] --> Fehler: [-59058.765625]\n",
      "[169320.08] vs [83500.] --> Fehler: [-85820.078125]\n",
      "[227621.75] vs [252000.] --> Fehler: [24378.25]\n",
      "[217494.7] vs [137500.] --> Fehler: [-79994.703125]\n",
      "[219877.11] vs [197000.] --> Fehler: [-22877.109375]\n",
      "[144887.06] vs [92900.] --> Fehler: [-51987.0625]\n",
      "[209432.64] vs [160000.] --> Fehler: [-49432.640625]\n",
      "[119961.805] vs [136500.] --> Fehler: [16538.1953125]\n",
      "[164781.33] vs [146000.] --> Fehler: [-18781.328125]\n",
      "[147377.45] vs [129000.] --> Fehler: [-18377.453125]\n",
      "[227815.62] vs [176432.] --> Fehler: [-51383.625]\n",
      "[106724.12] vs [127000.] --> Fehler: [20275.8828125]\n",
      "[191114.58] vs [170000.] --> Fehler: [-21114.578125]\n",
      "[127083.79] vs [128000.] --> Fehler: [916.2109375]\n",
      "[187566.06] vs [157000.] --> Fehler: [-30566.0625]\n",
      "[115021.71] vs [60000.] --> Fehler: [-55021.7109375]\n",
      "[174555.67] vs [119500.] --> Fehler: [-55055.671875]\n",
      "[154726.02] vs [135000.] --> Fehler: [-19726.015625]\n",
      "[155228.94] vs [159500.] --> Fehler: [4271.0625]\n",
      "[164759.42] vs [106000.] --> Fehler: [-58759.421875]\n",
      "[212529.8] vs [325000.] --> Fehler: [112470.203125]\n",
      "[204362.84] vs [179900.] --> Fehler: [-24462.84375]\n",
      "[242690.3] vs [274725.] --> Fehler: [32034.703125]\n",
      "[217266.6] vs [181000.] --> Fehler: [-36266.59375]\n",
      "[226927.4] vs [280000.] --> Fehler: [53072.59375]\n",
      "[209523.6] vs [188000.] --> Fehler: [-21523.59375]\n",
      "[183001.88] vs [205000.] --> Fehler: [21998.125]\n",
      "[150227.23] vs [129900.] --> Fehler: [-20327.234375]\n",
      "[148442.69] vs [134500.] --> Fehler: [-13942.6875]\n",
      "[143931.58] vs [117000.] --> Fehler: [-26931.578125]\n",
      "[232304.39] vs [318000.] --> Fehler: [85695.609375]\n",
      "[220002.94] vs [184100.] --> Fehler: [-35902.9375]\n",
      "[168421.45] vs [130000.] --> Fehler: [-38421.453125]\n",
      "[161034.45] vs [140000.] --> Fehler: [-21034.453125]\n",
      "[153788.4] vs [133700.] --> Fehler: [-20088.40625]\n",
      "[114807.87] vs [118400.] --> Fehler: [3592.1328125]\n",
      "[227098.88] vs [212900.] --> Fehler: [-14198.875]\n",
      "[151411.44] vs [112000.] --> Fehler: [-39411.4375]\n",
      "[143266.95] vs [118000.] --> Fehler: [-25266.953125]\n",
      "[206118.53] vs [163900.] --> Fehler: [-42218.53125]\n",
      "[152777.3] vs [115000.] --> Fehler: [-37777.296875]\n",
      "[223340.06] vs [174000.] --> Fehler: [-49340.0625]\n",
      "[230352.66] vs [259000.] --> Fehler: [28647.34375]\n",
      "[227390.53] vs [215000.] --> Fehler: [-12390.53125]\n",
      "[185642.58] vs [140000.] --> Fehler: [-45642.578125]\n",
      "[121995.555] vs [135000.] --> Fehler: [13004.4453125]\n",
      "[206276.78] vs [93500.] --> Fehler: [-112776.78125]\n",
      "[81286.73] vs [117500.] --> Fehler: [36213.2734375]\n",
      "[211474.17] vs [239500.] --> Fehler: [28025.828125]\n",
      "[213418.72] vs [169000.] --> Fehler: [-44418.71875]\n",
      "[117484.58] vs [102000.] --> Fehler: [-15484.578125]\n",
      "[146535.39] vs [119000.] --> Fehler: [-27535.390625]\n",
      "[72525.17] vs [94000.] --> Fehler: [21474.828125]\n",
      "[183831.72] vs [196000.] --> Fehler: [12168.28125]\n",
      "[115538.38] vs [144000.] --> Fehler: [28461.6171875]\n",
      "[144011.33] vs [139000.] --> Fehler: [-5011.328125]\n",
      "[182814.06] vs [197500.] --> Fehler: [14685.9375]\n",
      "[228507.08] vs [424870.] --> Fehler: [196362.921875]\n",
      "[155324.97] vs [80000.] --> Fehler: [-75324.96875]\n",
      "[141670.4] vs [80000.] --> Fehler: [-61670.40625]\n",
      "[119554.266] vs [149000.] --> Fehler: [29445.734375]\n",
      "[197586.58] vs [180000.] --> Fehler: [-17586.578125]\n",
      "[141389.44] vs [174500.] --> Fehler: [33110.5625]\n",
      "[116735.336] vs [116900.] --> Fehler: [164.6640625]\n",
      "[115958.266] vs [143000.] --> Fehler: [27041.734375]\n",
      "[144474.97] vs [124000.] --> Fehler: [-20474.96875]\n",
      "[168156.9] vs [149900.] --> Fehler: [-18256.90625]\n",
      "[159511.08] vs [230000.] --> Fehler: [70488.921875]\n",
      "[120983.98] vs [120500.] --> Fehler: [-483.9765625]\n",
      "[169551.05] vs [201800.] --> Fehler: [32248.953125]\n",
      "[183309.19] vs [218000.] --> Fehler: [34690.8125]\n",
      "[164909.38] vs [179900.] --> Fehler: [14990.625]\n",
      "[224745.38] vs [230000.] --> Fehler: [5254.625]\n",
      "[232141.12] vs [235128.] --> Fehler: [2986.875]\n",
      "[178480.36] vs [185000.] --> Fehler: [6519.640625]\n",
      "[177322.39] vs [146000.] --> Fehler: [-31322.390625]\n",
      "[158569.92] vs [224000.] --> Fehler: [65430.078125]\n",
      "[168846.88] vs [129000.] --> Fehler: [-39846.875]\n",
      "[174838.03] vs [108959.] --> Fehler: [-65879.03125]\n",
      "[192129.84] vs [194000.] --> Fehler: [1870.15625]\n",
      "[232660.11] vs [233170.] --> Fehler: [509.890625]\n",
      "[232121.6] vs [245350.] --> Fehler: [13228.40625]\n",
      "[220242.61] vs [173000.] --> Fehler: [-47242.609375]\n",
      "[138158.6] vs [235000.] --> Fehler: [96841.40625]\n",
      "[239144.9] vs [625000.] --> Fehler: [385855.09375]\n",
      "[183896.8] vs [171000.] --> Fehler: [-12896.796875]\n",
      "[153917.83] vs [163000.] --> Fehler: [9082.171875]\n",
      "[222160.25] vs [171900.] --> Fehler: [-50260.25]\n",
      "[155741.89] vs [200500.] --> Fehler: [44758.109375]\n",
      "[139615.1] vs [239000.] --> Fehler: [99384.90625]\n",
      "[207775.69] vs [285000.] --> Fehler: [77224.3125]\n",
      "[191517.52] vs [119500.] --> Fehler: [-72017.515625]\n",
      "[114003.72] vs [115000.] --> Fehler: [996.28125]\n",
      "[114887.42] vs [154900.] --> Fehler: [40012.578125]\n",
      "[148512.89] vs [93000.] --> Fehler: [-55512.890625]\n",
      "[205230.33] vs [250000.] --> Fehler: [44769.671875]\n",
      "[226905.86] vs [392500.] --> Fehler: [165594.140625]\n",
      "[219192.86] vs [745000.] --> Fehler: [525807.140625]\n",
      "[118767.93] vs [120000.] --> Fehler: [1232.0703125]\n",
      "[199234.73] vs [186700.] --> Fehler: [-12534.734375]\n",
      "[121062.305] vs [104900.] --> Fehler: [-16162.3046875]\n",
      "[98718.77] vs [95000.] --> Fehler: [-3718.7734375]\n",
      "[212748.55] vs [262000.] --> Fehler: [49251.453125]\n",
      "[221267.39] vs [195000.] --> Fehler: [-26267.390625]\n",
      "[215130.7] vs [189000.] --> Fehler: [-26130.703125]\n",
      "[193091.02] vs [168000.] --> Fehler: [-25091.015625]\n",
      "[209950.16] vs [174000.] --> Fehler: [-35950.15625]\n",
      "[121834.914] vs [125000.] --> Fehler: [3165.0859375]\n",
      "[211929.52] vs [165000.] --> Fehler: [-46929.515625]\n",
      "[171064.94] vs [158000.] --> Fehler: [-13064.9375]\n",
      "[224906.11] vs [176000.] --> Fehler: [-48906.109375]\n",
      "[232870.22] vs [219210.] --> Fehler: [-13660.21875]\n",
      "[111772.055] vs [144000.] --> Fehler: [32227.9453125]\n",
      "[219908.3] vs [178000.] --> Fehler: [-41908.296875]\n",
      "[163850.45] vs [148000.] --> Fehler: [-15850.453125]\n",
      "[172588.36] vs [116050.] --> Fehler: [-56538.359375]\n",
      "[216689.97] vs [197900.] --> Fehler: [-18789.96875]\n",
      "[116215.6] vs [117000.] --> Fehler: [784.3984375]\n",
      "[219066.69] vs [213000.] --> Fehler: [-6066.6875]\n",
      "[181103.9] vs [153500.] --> Fehler: [-27603.90625]\n",
      "[208979.97] vs [271900.] --> Fehler: [62920.03125]\n",
      "[165964.36] vs [107000.] --> Fehler: [-58964.359375]\n",
      "[223015.97] vs [200000.] --> Fehler: [-23015.96875]\n",
      "[158610.48] vs [140000.] --> Fehler: [-18610.484375]\n",
      "[228738.64] vs [290000.] --> Fehler: [61261.359375]\n",
      "[208351.88] vs [189000.] --> Fehler: [-19351.875]\n",
      "[203188.66] vs [164000.] --> Fehler: [-39188.65625]\n",
      "[137237.38] vs [113000.] --> Fehler: [-24237.375]\n",
      "[165865.44] vs [145000.] --> Fehler: [-20865.4375]\n",
      "[161216.22] vs [134500.] --> Fehler: [-26716.21875]\n",
      "[164037.28] vs [125000.] --> Fehler: [-39037.28125]\n",
      "[184418.25] vs [112000.] --> Fehler: [-72418.25]\n",
      "[231698.7] vs [229456.] --> Fehler: [-2242.703125]\n",
      "[138326.62] vs [80500.] --> Fehler: [-57826.625]\n",
      "[165936.05] vs [91500.] --> Fehler: [-74436.046875]\n",
      "[161720.3] vs [115000.] --> Fehler: [-46720.296875]\n",
      "[168341.1] vs [134000.] --> Fehler: [-34341.09375]\n",
      "[146945.95] vs [143000.] --> Fehler: [-3945.953125]\n",
      "[149209.19] vs [137900.] --> Fehler: [-11309.1875]\n",
      "[231219.11] vs [184000.] --> Fehler: [-47219.109375]\n",
      "[155816.27] vs [145000.] --> Fehler: [-10816.265625]\n",
      "[234985.88] vs [214000.] --> Fehler: [-20985.875]\n",
      "[164399.31] vs [147000.] --> Fehler: [-17399.3125]\n",
      "[230301.19] vs [367294.] --> Fehler: [136992.8125]\n",
      "[155697.02] vs [127000.] --> Fehler: [-28697.015625]\n",
      "[193510.92] vs [190000.] --> Fehler: [-3510.921875]\n",
      "[158573.12] vs [132500.] --> Fehler: [-26073.125]\n",
      "[160828.89] vs [101800.] --> Fehler: [-59028.890625]\n",
      "[159423.53] vs [142000.] --> Fehler: [-17423.53125]\n",
      "[106324.66] vs [130000.] --> Fehler: [23675.34375]\n",
      "[116479.89] vs [138887.] --> Fehler: [22407.109375]\n",
      "[216072.67] vs [175500.] --> Fehler: [-40572.671875]\n",
      "[228027.56] vs [195000.] --> Fehler: [-33027.5625]\n",
      "[230287.25] vs [142500.] --> Fehler: [-87787.25]\n",
      "[227516.84] vs [265900.] --> Fehler: [38383.15625]\n",
      "[221973.45] vs [224900.] --> Fehler: [2926.546875]\n",
      "[229918.47] vs [248328.] --> Fehler: [18409.53125]\n",
      "[180086.3] vs [170000.] --> Fehler: [-10086.296875]\n",
      "[232696.31] vs [465000.] --> Fehler: [232303.6875]\n",
      "[128651.29] vs [230000.] --> Fehler: [101348.7109375]\n",
      "[197001.08] vs [178000.] --> Fehler: [-19001.078125]\n",
      "[225008.52] vs [186500.] --> Fehler: [-38508.515625]\n",
      "[184973.78] vs [169900.] --> Fehler: [-15073.78125]\n",
      "[113930.77] vs [129500.] --> Fehler: [15569.2265625]\n",
      "[142789.17] vs [119000.] --> Fehler: [-23789.171875]\n",
      "[171446.22] vs [244000.] --> Fehler: [72553.78125]\n",
      "[216614.7] vs [171750.] --> Fehler: [-44864.703125]\n",
      "[170056.9] vs [130000.] --> Fehler: [-40056.90625]\n",
      "[187467.08] vs [294000.] --> Fehler: [106532.921875]\n",
      "[220664.16] vs [165400.] --> Fehler: [-55264.15625]\n",
      "[122518.32] vs [127500.] --> Fehler: [4981.6796875]\n",
      "[214714.06] vs [301500.] --> Fehler: [86785.9375]\n",
      "[110223.336] vs [99900.] --> Fehler: [-10323.3359375]\n",
      "[226568.56] vs [190000.] --> Fehler: [-36568.5625]\n",
      "[171476.83] vs [151000.] --> Fehler: [-20476.828125]\n",
      "[233464.75] vs [181000.] --> Fehler: [-52464.75]\n",
      "[152463.48] vs [128900.] --> Fehler: [-23563.484375]\n",
      "[156027.05] vs [161500.] --> Fehler: [5472.953125]\n",
      "[122017.81] vs [180500.] --> Fehler: [58482.1875]\n",
      "[209924.88] vs [181000.] --> Fehler: [-28924.875]\n",
      "[211113.25] vs [183900.] --> Fehler: [-27213.25]\n",
      "[107826.31] vs [122000.] --> Fehler: [14173.6875]\n",
      "[235044.23] vs [378500.] --> Fehler: [143455.765625]\n",
      "[138739.27] vs [381000.] --> Fehler: [242260.734375]\n",
      "[157161.78] vs [144000.] --> Fehler: [-13161.78125]\n",
      "[201601.72] vs [260000.] --> Fehler: [58398.28125]\n",
      "[169307.83] vs [185750.] --> Fehler: [16442.171875]\n",
      "[169365.36] vs [137000.] --> Fehler: [-32365.359375]\n",
      "[158412.06] vs [177000.] --> Fehler: [18587.9375]\n",
      "[100399.44] vs [139000.] --> Fehler: [38600.5625]\n",
      "[147283.16] vs [137000.] --> Fehler: [-10283.15625]\n",
      "[179481.92] vs [162000.] --> Fehler: [-17481.921875]\n",
      "[177072.02] vs [197900.] --> Fehler: [20827.984375]\n",
      "[221841.47] vs [237000.] --> Fehler: [15158.53125]\n",
      "[113616.875] vs [68400.] --> Fehler: [-45216.875]\n",
      "[222198.89] vs [227000.] --> Fehler: [4801.109375]\n",
      "[201900.08] vs [180000.] --> Fehler: [-21900.078125]\n",
      "[182744.36] vs [150500.] --> Fehler: [-32244.359375]\n",
      "[174173.67] vs [139000.] --> Fehler: [-35173.671875]\n",
      "[115966.13] vs [169000.] --> Fehler: [53033.8671875]\n",
      "[130047.85] vs [132500.] --> Fehler: [2452.1484375]\n",
      "[162308.56] vs [143000.] --> Fehler: [-19308.5625]\n",
      "[202356.53] vs [190000.] --> Fehler: [-12356.53125]\n",
      "[214706.75] vs [278000.] --> Fehler: [63293.25]\n",
      "[229680.83] vs [281000.] --> Fehler: [51319.171875]\n",
      "[168455.52] vs [180500.] --> Fehler: [12044.484375]\n",
      "[167471.19] vs [119500.] --> Fehler: [-47971.1875]\n",
      "[84547.516] vs [107500.] --> Fehler: [22952.484375]\n",
      "[182639.06] vs [162900.] --> Fehler: [-19739.0625]\n",
      "[149246.47] vs [115000.] --> Fehler: [-34246.46875]\n",
      "[168501.14] vs [138500.] --> Fehler: [-30001.140625]\n",
      "[161145.48] vs [155000.] --> Fehler: [-6145.484375]\n",
      "[220260.14] vs [140000.] --> Fehler: [-80260.140625]\n",
      "[277253.72] vs [160000.] --> Fehler: [-117253.71875]\n",
      "[153724.39] vs [154000.] --> Fehler: [275.609375]\n",
      "[218611.4] vs [225000.] --> Fehler: [6388.59375]\n",
      "[135353.3] vs [177500.] --> Fehler: [42146.703125]\n",
      "[210250.6] vs [290000.] --> Fehler: [79749.40625]\n",
      "[225609.3] vs [232000.] --> Fehler: [6390.703125]\n",
      "[218392.1] vs [130000.] --> Fehler: [-88392.09375]\n",
      "[231930.19] vs [325000.] --> Fehler: [93069.8125]\n",
      "[223760.1] vs [202500.] --> Fehler: [-21260.09375]\n",
      "[208065.28] vs [138000.] --> Fehler: [-70065.28125]\n",
      "[148305.56] vs [147000.] --> Fehler: [-1305.5625]\n",
      "[202479.17] vs [179200.] --> Fehler: [-23279.171875]\n",
      "[167758.86] vs [335000.] --> Fehler: [167241.140625]\n",
      "[225743.75] vs [203000.] --> Fehler: [-22743.75]\n",
      "[203525.2] vs [302000.] --> Fehler: [98474.796875]\n",
      "[222892.42] vs [333168.] --> Fehler: [110275.578125]\n",
      "[148286.56] vs [119000.] --> Fehler: [-29286.5625]\n",
      "[172890.66] vs [206900.] --> Fehler: [34009.34375]\n",
      "[231855.89] vs [295493.] --> Fehler: [63637.109375]\n",
      "[222387.53] vs [208900.] --> Fehler: [-13487.53125]\n",
      "[225970.2] vs [275000.] --> Fehler: [49029.796875]\n",
      "[151447.42] vs [111000.] --> Fehler: [-40447.421875]\n",
      "[151614.4] vs [156500.] --> Fehler: [4885.59375]\n",
      "[140906.73] vs [72500.] --> Fehler: [-68406.734375]\n",
      "[207250.67] vs [190000.] --> Fehler: [-17250.671875]\n",
      "[129990.055] vs [82500.] --> Fehler: [-47490.0546875]\n",
      "[228529.47] vs [147000.] --> Fehler: [-81529.46875]\n",
      "[109561.516] vs [55000.] --> Fehler: [-54561.515625]\n",
      "[119443.3] vs [79000.] --> Fehler: [-40443.296875]\n",
      "[188072.66] vs [130500.] --> Fehler: [-57572.65625]\n",
      "[118205.99] vs [256000.] --> Fehler: [137794.0078125]\n",
      "[215285.73] vs [176500.] --> Fehler: [-38785.734375]\n",
      "[228544.42] vs [227000.] --> Fehler: [-1544.421875]\n",
      "[183322.] vs [132500.] --> Fehler: [-50822.]\n",
      "[133550.61] vs [100000.] --> Fehler: [-33550.609375]\n",
      "[130932.945] vs [125500.] --> Fehler: [-5432.9453125]\n",
      "[165135.02] vs [125000.] --> Fehler: [-40135.015625]\n",
      "[183651.38] vs [167900.] --> Fehler: [-15751.375]\n",
      "[177079.66] vs [135000.] --> Fehler: [-42079.65625]\n",
      "[129086.234] vs [52500.] --> Fehler: [-76586.234375]\n",
      "[226085.12] vs [200000.] --> Fehler: [-26085.125]\n",
      "[175879.55] vs [128500.] --> Fehler: [-47379.546875]\n",
      "[172993.5] vs [123000.] --> Fehler: [-49993.5]\n",
      "[227881.7] vs [155000.] --> Fehler: [-72881.703125]\n",
      "[221736.89] vs [228500.] --> Fehler: [6763.109375]\n",
      "[121611.57] vs [177000.] --> Fehler: [55388.4296875]\n",
      "[229721.38] vs [155835.] --> Fehler: [-73886.375]\n",
      "[111275.5] vs [108500.] --> Fehler: [-2775.5]\n",
      "[181772.4] vs [262500.] --> Fehler: [80727.59375]\n",
      "[234206.19] vs [283463.] --> Fehler: [49256.8125]\n",
      "[222874.61] vs [215000.] --> Fehler: [-7874.609375]\n",
      "[70895.63] vs [122000.] --> Fehler: [51104.3671875]\n",
      "[173496.75] vs [200000.] --> Fehler: [26503.25]\n",
      "[160193.98] vs [171000.] --> Fehler: [10806.015625]\n",
      "[128071.82] vs [134900.] --> Fehler: [6828.1796875]\n",
      "[216694.17] vs [410000.] --> Fehler: [193305.828125]\n",
      "[219670.66] vs [235000.] --> Fehler: [15329.34375]\n",
      "[170413.28] vs [170000.] --> Fehler: [-413.28125]\n",
      "[166580.05] vs [110000.] --> Fehler: [-56580.046875]\n",
      "[177521.] vs [149900.] --> Fehler: [-27621.]\n",
      "[210921.92] vs [177500.] --> Fehler: [-33421.921875]\n",
      "[232662.83] vs [315000.] --> Fehler: [82337.171875]\n",
      "[118260.586] vs [189000.] --> Fehler: [70739.4140625]\n",
      "[233580.17] vs [260000.] --> Fehler: [26419.828125]\n",
      "[121441.79] vs [104900.] --> Fehler: [-16541.7890625]\n",
      "[226942.77] vs [156932.] --> Fehler: [-70010.765625]\n",
      "[219731.97] vs [144152.] --> Fehler: [-75579.96875]\n",
      "[216665.84] vs [216000.] --> Fehler: [-665.84375]\n",
      "[216922.25] vs [193000.] --> Fehler: [-23922.25]\n",
      "[176198.03] vs [127000.] --> Fehler: [-49198.03125]\n",
      "[218000.8] vs [144000.] --> Fehler: [-74000.796875]\n",
      "[224616.55] vs [232000.] --> Fehler: [7383.453125]\n",
      "[110338.945] vs [105000.] --> Fehler: [-5338.9453125]\n",
      "[151475.47] vs [165500.] --> Fehler: [14024.53125]\n",
      "[215996.38] vs [274300.] --> Fehler: [58303.625]\n",
      "[222362.52] vs [466500.] --> Fehler: [244137.484375]\n",
      "[227676.17] vs [250000.] --> Fehler: [22323.828125]\n",
      "[231137.05] vs [239000.] --> Fehler: [7862.953125]\n",
      "[121611.48] vs [91000.] --> Fehler: [-30611.4765625]\n",
      "[139825.39] vs [117000.] --> Fehler: [-22825.390625]\n",
      "[169297.66] vs [83000.] --> Fehler: [-86297.65625]\n",
      "[228261.64] vs [167500.] --> Fehler: [-60761.640625]\n",
      "[108800.16] vs [58500.] --> Fehler: [-50300.15625]\n",
      "[176399.86] vs [237500.] --> Fehler: [61100.140625]\n",
      "[113148.59] vs [157000.] --> Fehler: [43851.40625]\n",
      "[139486.17] vs [112000.] --> Fehler: [-27486.171875]\n",
      "[134824.28] vs [105000.] --> Fehler: [-29824.28125]\n",
      "[112371.17] vs [125500.] --> Fehler: [13128.828125]\n",
      "[192700.69] vs [250000.] --> Fehler: [57299.3125]\n",
      "[111256.945] vs [136000.] --> Fehler: [24743.0546875]\n",
      "[233764.42] vs [377500.] --> Fehler: [143735.578125]\n",
      "[132023.89] vs [131000.] --> Fehler: [-1023.890625]\n",
      "[218373.12] vs [235000.] --> Fehler: [16626.875]\n",
      "[167546.45] vs [124000.] --> Fehler: [-43546.453125]\n",
      "[166366.3] vs [123000.] --> Fehler: [-43366.296875]\n",
      "[103947.64] vs [163000.] --> Fehler: [59052.359375]\n",
      "[222190.14] vs [246578.] --> Fehler: [24387.859375]\n",
      "[229852.94] vs [281213.] --> Fehler: [51360.0625]\n",
      "[211940.72] vs [160000.] --> Fehler: [-51940.71875]\n",
      "[111462.805] vs [137500.] --> Fehler: [26037.1953125]\n",
      "[142789.17] vs [138000.] --> Fehler: [-4789.171875]\n",
      "[116482.5] vs [137450.] --> Fehler: [20967.5]\n",
      "[120167.66] vs [120000.] --> Fehler: [-167.65625]\n",
      "[222715.78] vs [193000.] --> Fehler: [-29715.78125]\n",
      "[225089.3] vs [193879.] --> Fehler: [-31210.296875]\n",
      "[235687.98] vs [282922.] --> Fehler: [47234.015625]\n",
      "[113219.086] vs [105000.] --> Fehler: [-8219.0859375]\n",
      "[218903.22] vs [275000.] --> Fehler: [56096.78125]\n",
      "[174689.78] vs [133000.] --> Fehler: [-41689.78125]\n",
      "[195004.3] vs [112000.] --> Fehler: [-83004.296875]\n",
      "[104111.31] vs [125500.] --> Fehler: [21388.6875]\n",
      "[209092.23] vs [215000.] --> Fehler: [5907.765625]\n",
      "[223450.89] vs [230000.] --> Fehler: [6549.109375]\n",
      "[146535.39] vs [140000.] --> Fehler: [-6535.390625]\n",
      "[141801.16] vs [90000.] --> Fehler: [-51801.15625]\n",
      "[228069.92] vs [257000.] --> Fehler: [28930.078125]\n",
      "[125248.76] vs [207000.] --> Fehler: [81751.2421875]\n",
      "[223287.81] vs [175900.] --> Fehler: [-47387.8125]\n",
      "[86655.555] vs [122500.] --> Fehler: [35844.4453125]\n",
      "[223247.02] vs [340000.] --> Fehler: [116752.984375]\n",
      "[161683.27] vs [124000.] --> Fehler: [-37683.265625]\n",
      "[178552.42] vs [223000.] --> Fehler: [44447.578125]\n",
      "[172022.42] vs [179900.] --> Fehler: [7877.578125]\n",
      "[177668.42] vs [127500.] --> Fehler: [-50168.421875]\n",
      "[218000.8] vs [136500.] --> Fehler: [-81500.796875]\n",
      "[178093.4] vs [274970.] --> Fehler: [96876.59375]\n",
      "[154326.52] vs [144000.] --> Fehler: [-10326.515625]\n",
      "[157177.36] vs [142000.] --> Fehler: [-15177.359375]\n",
      "[211129.81] vs [271000.] --> Fehler: [59870.1875]\n",
      "[143671.31] vs [140000.] --> Fehler: [-3671.3125]\n",
      "[132908.98] vs [119000.] --> Fehler: [-13908.984375]\n",
      "[192882.19] vs [182900.] --> Fehler: [-9982.1875]\n",
      "[239739.17] vs [192140.] --> Fehler: [-47599.171875]\n",
      "[177077.6] vs [143750.] --> Fehler: [-33327.59375]\n",
      "[125684.06] vs [64500.] --> Fehler: [-61184.0625]\n",
      "[219611.97] vs [186500.] --> Fehler: [-33111.96875]\n",
      "[191921.03] vs [160000.] --> Fehler: [-31921.03125]\n",
      "[159290.2] vs [174000.] --> Fehler: [14709.796875]\n",
      "[173746.84] vs [120500.] --> Fehler: [-53246.84375]\n",
      "[234222.61] vs [394617.] --> Fehler: [160394.390625]\n",
      "[150554.81] vs [149700.] --> Fehler: [-854.8125]\n",
      "[188785.36] vs [197000.] --> Fehler: [8214.640625]\n",
      "[121877.19] vs [191000.] --> Fehler: [69122.8125]\n",
      "[219526.36] vs [149300.] --> Fehler: [-70226.359375]\n",
      "[232684.98] vs [310000.] --> Fehler: [77315.015625]\n",
      "[111778.29] vs [121000.] --> Fehler: [9221.7109375]\n",
      "[223873.53] vs [179600.] --> Fehler: [-44273.53125]\n",
      "[165430.83] vs [129000.] --> Fehler: [-36430.828125]\n",
      "[184212.4] vs [157900.] --> Fehler: [-26312.40625]\n",
      "[211657.7] vs [240000.] --> Fehler: [28342.296875]\n",
      "[110397.15] vs [112000.] --> Fehler: [1602.8515625]\n",
      "[164244.02] vs [92000.] --> Fehler: [-72244.015625]\n",
      "[178352.33] vs [136000.] --> Fehler: [-42352.328125]\n",
      "[230827.25] vs [287090.] --> Fehler: [56262.75]\n",
      "[220260.14] vs [145000.] --> Fehler: [-75260.140625]\n",
      "[236245.3] vs [84500.] --> Fehler: [-151745.296875]\n",
      "[222806.48] vs [185000.] --> Fehler: [-37806.484375]\n",
      "[215575.64] vs [175000.] --> Fehler: [-40575.640625]\n",
      "[188947.89] vs [210000.] --> Fehler: [21052.109375]\n",
      "[136772.22] vs [266500.] --> Fehler: [129727.78125]\n",
      "[146718.] vs [142125.] --> Fehler: [-4593.]\n",
      "[165535.72] vs [147500.] --> Fehler: [-18035.71875]\n",
      "Durchschnittlicher Fehler in $: [42763.99684103]\n"
     ]
    }
   ],
   "source": [
    "nr_tests = len(y_test)\n",
    "sum_errors = 0.0\n",
    "for i in range(0,nr_tests):\n",
    "    error = gt_dollar[i] - preds_dollar[i]\n",
    "    print(\"{0} vs {1} --> Fehler: {2}\"\n",
    "          .format(preds_dollar[i],\n",
    "                  gt_dollar[i],\n",
    "                  error ))\n",
    "    sum_errors += abs(error)\n",
    "print(\"Durchschnittlicher Fehler in $:\", sum_errors/nr_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname1 = \"hauspreis_schaetzer.h5\"\n",
    "model.save(fname1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fname2 = \"scaler_output.pkl\"\n",
    "fobj = open(fname2, \"wb\")\n",
    "pickle.dump(scaler_output, fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell wiederherstellen und anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model(fname1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fobj = open(fname2, \"rb\")\n",
    "scaler = pickle.load(fobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.preprocessing._data.MinMaxScaler"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = np.array( [[0.94927536, 0.0334198 ]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict( test_sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26022083]], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[222285.02]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.inverse_transform( pred )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Inhalte",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
